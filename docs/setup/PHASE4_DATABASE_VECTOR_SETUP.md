# Phase 4: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ™ã‚¯ãƒˆãƒ«ç’°å¢ƒæ§‹ç¯‰

## ğŸ“‹ ã“ã®ãƒ•ã‚§ãƒ¼ã‚ºã®æ¦‚è¦

### ç›®çš„

AutoForgeNexusã®æ°¸ç¶šåŒ–å±¤ã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢åŸºç›¤ã‚’æ§‹ç¯‰ã—ã€é«˜æ€§èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã¨AIæ©Ÿèƒ½ã®ãŸã‚ã®åŸ‹ã‚è¾¼ã¿å‡¦ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

#### **ä¸»è¦æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**

- **edge-database-administrator** (ãƒªãƒ¼ãƒ€ãƒ¼):
  Turso/libSQLè¨­è¨ˆãƒ»æœ€é©åŒ–ã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†
- **vector-database-specialist**: libSQL
  Vectorç®¡ç†ã€åŸ‹ã‚è¾¼ã¿æˆ¦ç•¥ã€é¡ä¼¼åº¦æ¤œç´¢æœ€é©åŒ–
- **data-migration-specialist**: ãƒ‡ãƒ¼ã‚¿ç§»è¡Œæˆ¦ç•¥ã€ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã‚¼ãƒ­ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ç§»è¡Œ

#### **æ”¯æ´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**

- **backend-architect**: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§è¨­è¨ˆã€éšœå®³è¨±å®¹æ€§ã€ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹å±¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **performance-optimizer**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã€ã‚¯ã‚¨ãƒªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- **security-architect**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€æš—å·åŒ–ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
- **sre-agent-agent**: é‹ç”¨ç›£è¦–ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥ã€éšœå®³å¯¾å¿œ
- **devops-coordinator**: Dockerçµ±åˆã€CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ç’°å¢ƒç®¡ç†

### é–¢é€£AIã‚³ãƒãƒ³ãƒ‰

- `/ai:data:vector` - libSQL Vectorã«ã‚ˆã‚‹ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†
- `/ai:data:analyze` - ãƒ‡ãƒ¼ã‚¿åˆ†æã¨æ´å¯ŸæŠ½å‡º
- `/ai:data:migrate` - ã‚¼ãƒ­ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
- `/ai:operations:monitor` - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–
- `/ai:troubleshoot` - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å•é¡Œè¨ºæ–­

### æœ€çµ‚çŠ¶æ…‹

- âœ… Turso (libSQL)ã«ã‚ˆã‚‹åˆ†æ•£å‹ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç¨¼åƒ
- âœ… Redisã«ã‚ˆã‚‹é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹ç¯‰
- âœ… libSQL Vectorã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç’°å¢ƒæ•´å‚™
- âœ… SQLAlchemy ORMã¨Alembicãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
- âœ… é–‹ç™ºãƒ»æœ¬ç•ªç’°å¢ƒã®åˆ†é›¢ã¨ãƒ–ãƒ©ãƒ³ãƒãƒ³ã‚°æˆ¦ç•¥ç¢ºç«‹
- âœ… è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½“åˆ¶æ§‹ç¯‰

### æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

```yaml
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹:
  ãƒ¡ã‚¤ãƒ³: Turso (libSQL) - åˆ†æ•£SQLite
  ã‚­ãƒ£ãƒƒã‚·ãƒ¥: Redis 7.4.1
  ãƒ™ã‚¯ãƒˆãƒ«: libSQL Vector Extension

ORM/ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:
  ORM: SQLAlchemy 2.0.32
  ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: Alembic 1.14.0

ç®¡ç†ãƒ„ãƒ¼ãƒ«:
  CLI: Turso CLI 0.97.1
  ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°: Redis Insight
  ãƒ™ã‚¯ãƒˆãƒ«ç®¡ç†: pgvector-python 0.3.7
```

---

## ğŸš€ äº‹å‰æº–å‚™ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å¿…é ˆç¢ºèªé …ç›®

```bash
# Phase 3å®Œäº†ç¢ºèª
cat backend/.env.local | grep DATABASE_URL  # ç’°å¢ƒå¤‰æ•°æº–å‚™ç¢ºèª
docker --version  # Docker 24.0ä»¥ä¸Š
docker-compose --version  # Docker Compose 2.20ä»¥ä¸Š
python --version  # Python 3.13ç¢ºèª

# M1 Macå›ºæœ‰ã®ç¢ºèª
uname -m  # å‡ºåŠ›: arm64 ã‚’ç¢ºèª
arch  # å‡ºåŠ›: arm64 ã‚’ç¢ºèª
softwareupdate --list  # macOSæœ€æ–°ç‰ˆç¢ºèª

# Rosetta 2ç¢ºèªï¼ˆx86_64äº’æ›ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼‰
/usr/bin/pgrep oahd >/dev/null 2>&1 && echo "Rosetta 2: Installed" || echo "Rosetta 2: Not installed"
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ãªå ´åˆ:
# softwareupdate --install-rosetta --agree-to-license

# åˆ©ç”¨å¯èƒ½ãƒªã‚½ãƒ¼ã‚¹ç¢ºèªï¼ˆmacOSç”¨ï¼‰
df -h  # 20GBä»¥ä¸Šã®ç©ºãå®¹é‡
sysctl hw.memsize | awk '{print $2/1073741824 " GB"}'  # 8GBä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒª
sysctl -n hw.ncpu  # CPU ã‚³ã‚¢æ•°ç¢ºèª

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª
curl -I https://api.turso.tech  # Turso APIæ¥ç¶šç¢ºèª
```

### ç’°å¢ƒå¤‰æ•°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæº–å‚™

```bash
# backend/.env.localã«è¿½åŠ ï¼ˆã‚»ã‚­ãƒ¥ã‚¢ãªè¨­å®šï¼‰
cat >> backend/.env.local << 'EOF'

# === Database Configuration ===
# Turso (Primary Database)
TURSO_DATABASE_URL="libsql://[database]-[organization].turso.io"
TURSO_AUTH_TOKEN="[your-auth-token]"
TURSO_SYNC_URL="https://[database]-[organization].turso.io"

# Redis Cache (ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–)
REDIS_URL="redis://localhost:6379/0"
REDIS_PASSWORD="$(openssl rand -base64 32)"  # è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹å¼·åŠ›ãªãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
REDIS_SSL="false"  # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º
REDIS_DB=0
REDIS_MAX_RETRIES=3
REDIS_RETRY_DELAY=1000

# Vector Database
VECTOR_DIMENSION=1536  # OpenAIåŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚º
VECTOR_INDEX_TYPE="hnsw"  # é«˜é€Ÿè¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢
VECTOR_METRIC="cosine"  # é¡ä¼¼åº¦ãƒ¡ãƒˆãƒªãƒƒã‚¯

# Database Settings
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40
DB_POOL_TIMEOUT=30
DB_POOL_RECYCLE=3600
DB_ECHO=false  # é–‹ç™ºæ™‚ã¯trueã§ SQL ãƒ­ã‚°å‡ºåŠ›

# Migration Settings
ALEMBIC_CONFIG="backend/alembic.ini"
ALEMBIC_MIGRATION_DIR="backend/migrations"
EOF
```

### Claude Agentã®æ´»ç”¨æº–å‚™

```bash
# é–¢é€£ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç¢ºèª
/ai:data:vector --help  # ãƒ™ã‚¯ãƒˆãƒ«DBç®¡ç†
/ai:data:migrate --help  # ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
/ai:operations:deploy --help  # ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®š
```

---

## 1ï¸âƒ£ Turso (libSQL)ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

- **ä¸»æ‹…å½“**: edge-database-administrator
- **æ”¯æ´**: backend-architect, devops-coordinator

### èƒŒæ™¯ãƒ»ç›®çš„

Turso
(libSQL)ã¯åˆ†æ•£SQLiteãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã€ã‚¨ãƒƒã‚¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚¢ã‚¯ã‚»ã‚¹ã€SQLiteã®ä¿¡é ¼æ€§ã‚’çµ„ã¿åˆã‚ã›ãŸæ¬¡ä¸–ä»£ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚

### ä½¿ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã¨æ§‹é€ ç¢ºèª
/ai:architecture:design --database-structure

# åˆæœŸã‚¹ã‚­ãƒ¼ãƒã®æœ€é©åŒ–
/ai:data:analyze --schema-optimization
```

### 1.1 Turso CLIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆM1 Macå¯¾å¿œï¼‰

```bash
# M1 Mac (Apple Silicon)
# ARM64ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ“ãƒ«ãƒ‰ã‚’å„ªå…ˆ
brew install tursodatabase/tap/turso

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç¢ºèª
file $(which turso)
# æœŸå¾…å‡ºåŠ›: Mach-O 64-bit executable arm64

# ã‚‚ã—x86_64ç‰ˆãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸå ´åˆ
brew uninstall turso
brew install --build-from-source tursodatabase/tap/turso

# Linux
curl -sSfL https://get.tur.so/install.sh | bash

# Windows (WSL2)
curl -sSfL https://get.tur.so/install.sh | bash

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
turso --version
# æœŸå¾…å‡ºåŠ›: turso 0.97.1 (arm64)
```

### 1.2 Tursoã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®š

```bash
# ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³
turso auth signup  # æ–°è¦ã®å ´åˆ
# ã¾ãŸã¯
turso auth login  # æ—¢å­˜ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ

# èªè¨¼ç¢ºèª
turso account show
# æœŸå¾…å‡ºåŠ›:
# Email: your-email@example.com
# Plan: Starter (ã¾ãŸã¯ä»–ã®ãƒ—ãƒ©ãƒ³)
```

### 1.3 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ

```bash
# æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
turso db create autoforgenexus-prod \
  --location nrt  # æ±äº¬ãƒªãƒ¼ã‚¸ãƒ§ãƒ³

# é–‹ç™ºãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆï¼ˆæœ¬ç•ªã®ãƒ¬ãƒ—ãƒªã‚«ï¼‰
turso db create autoforgenexus-dev \
  --from-db autoforgenexus-prod

# ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒ
turso db create autoforgenexus-staging \
  --from-db autoforgenexus-prod

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§ç¢ºèª
turso db list
# æœŸå¾…å‡ºåŠ›:
# NAME                    LOCATIONS  SIZE
# autoforgenexus-prod     nrt        0 B
# autoforgenexus-dev      nrt        0 B
# autoforgenexus-staging  nrt        0 B
```

### 1.4 æ¥ç¶šæƒ…å ±å–å¾—

```bash
# æœ¬ç•ªDBæ¥ç¶šæƒ…å ±
turso db show autoforgenexus-prod --url
turso db tokens create autoforgenexus-prod

# é–‹ç™ºDBæ¥ç¶šæƒ…å ±
turso db show autoforgenexus-dev --url
turso db tokens create autoforgenexus-dev

# æ¥ç¶šãƒ†ã‚¹ãƒˆ
turso db shell autoforgenexus-dev "SELECT 1"
# æœŸå¾…å‡ºåŠ›: 1
```

### 1.5 libSQL Vector Extensionæœ‰åŠ¹åŒ–ï¼ˆä¿®æ­£ç‰ˆï¼‰

```bash
# Vectoræ‹¡å¼µã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
turso db shell autoforgenexus-dev << 'EOF'
-- Vectoræ‹¡å¼µæœ‰åŠ¹åŒ–ï¼ˆæ­£ã—ã„æ§‹æ–‡ï¼‰
CREATE VIRTUAL TABLE IF NOT EXISTS vec_items
USING vec0(
  embedding FLOAT[1536]
);

-- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ã‚’åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã§ç®¡ç†
CREATE TABLE IF NOT EXISTS vec_items_metadata (
    id TEXT PRIMARY KEY,
    vec_id INTEGER REFERENCES vec_items(rowid),
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- HNSWãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
CREATE TABLE IF NOT EXISTS vec_config (
    key TEXT PRIMARY KEY,
    value TEXT
);

INSERT OR REPLACE INTO vec_config VALUES
    ('hnsw_m', '16'),  -- æ¥ç¶šæ•°ï¼ˆç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
    ('hnsw_ef_construction', '200'),  -- æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…
    ('hnsw_ef_search', '50');  -- æ¤œç´¢æ™‚ã®æ¢ç´¢å¹…

-- ç¢ºèª
SELECT name, sql FROM sqlite_master
WHERE type='table' AND name LIKE 'vec_%';
EOF
```

### 1.6 åˆæœŸã‚¹ã‚­ãƒ¼ãƒè¨­å®š

```bash
# backend/sql/001_initial_schema.sqlä½œæˆ
cat > backend/sql/001_initial_schema.sql << 'EOF'
-- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS users (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    clerk_id TEXT UNIQUE NOT NULL,
    email TEXT UNIQUE NOT NULL,
    username TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSON
);

-- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS prompts (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    version INTEGER DEFAULT 1,
    status TEXT CHECK(status IN ('draft', 'active', 'archived')) DEFAULT 'draft',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSON
);

-- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS prompt_embeddings (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    prompt_id TEXT NOT NULL REFERENCES prompts(id) ON DELETE CASCADE,
    embedding BLOB NOT NULL,  -- 1536æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«
    model TEXT DEFAULT 'text-embedding-ada-002',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- è©•ä¾¡çµæœãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS evaluations (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    prompt_id TEXT NOT NULL REFERENCES prompts(id) ON DELETE CASCADE,
    score REAL NOT NULL,
    metrics JSON NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
CREATE INDEX idx_users_clerk_id ON users(clerk_id);
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_prompts_user_id ON prompts(user_id);
CREATE INDEX idx_prompts_status ON prompts(status);
CREATE INDEX idx_prompt_embeddings_prompt_id ON prompt_embeddings(prompt_id);
CREATE INDEX idx_evaluations_prompt_id ON evaluations(prompt_id);
EOF

# ã‚¹ã‚­ãƒ¼ãƒé©ç”¨
turso db shell autoforgenexus-dev < backend/sql/001_initial_schema.sql
```

---

## 2ï¸âƒ£ Redisç’°å¢ƒæ§‹ç¯‰

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

- **ä¸»æ‹…å½“**: backend-architect
- **æ”¯æ´**: performance-optimizer, sre-agent-agent

### èƒŒæ™¯ãƒ»ç›®çš„

Redisã¯é«˜é€Ÿã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã§ã€ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿèƒ½ã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚Pub/Subæ©Ÿèƒ½ã«ã‚ˆã‚Šã€ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Ÿè£…ã‚‚å¯èƒ½ã§ã™ã€‚

### ä½¿ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®è¨­è¨ˆ
/ai:architecture:design --cache-strategy

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
/ai:operations:monitor --redis-metrics
```

### 2.1 Redisã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆM1 Macæœ€é©åŒ–ï¼‰

```bash
# M1 Mac (ARM64ãƒã‚¤ãƒ†ã‚£ãƒ–)
brew install redis

# ARM64æœ€é©åŒ–è¨­å®šã‚’è¿½åŠ 
echo "io-threads 4" >> /opt/homebrew/etc/redis.conf
echo "io-threads-do-reads yes" >> /opt/homebrew/etc/redis.conf

# ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•
brew services start redis

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
redis-server --version
# æœŸå¾…å‡ºåŠ›: Redis server v=7.4.1 ... (arm64)

# Linux (Ubuntu/Debian)
sudo apt update
sudo apt install redis-server -y
sudo systemctl enable redis-server
sudo systemctl start redis-server

# å‹•ä½œç¢ºèª
redis-cli ping
# æœŸå¾…å‡ºåŠ›: PONG

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆM1 Macç”¨ï¼‰
redis-benchmark -q -n 100000
# M1ã§ã®æœŸå¾…å€¤: SET: 100000+ requests per second
```

### 2.2 Redisè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚»ã‚­ãƒ¥ã‚¢ç‰ˆï¼‰

```bash
# backend/config/redis.confä½œæˆ
cat > backend/config/redis.conf << 'EOF'
# åŸºæœ¬è¨­å®šï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ï¼‰
bind 127.0.0.1 ::1
protected-mode yes
port 6379
tcp-backlog 511
timeout 0
tcp-keepalive 300

# èªè¨¼è¨­å®šï¼ˆå¿…é ˆï¼‰
requirepass $(openssl rand -base64 32)

# ACLè¨­å®š
aclfile /opt/homebrew/etc/redis-acl.conf

# ãƒ¡ãƒ¢ãƒªç®¡ç†
maxmemory 2gb
maxmemory-policy allkeys-lru

# æ°¸ç¶šåŒ–è¨­å®šï¼ˆé–‹ç™ºç’°å¢ƒï¼‰
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir ./data/redis

# ãƒ­ã‚°è¨­å®š
loglevel notice
logfile ./logs/redis.log

# ã‚¹ãƒ­ãƒ¼ãƒ­ã‚°
slowlog-log-slower-than 10000
slowlog-max-len 128

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç†
maxclients 10000

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
replica-lazy-flush no
EOF

# Redisèµ·å‹•ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šï¼‰
redis-server backend/config/redis.conf
```

### 2.3 Redisæ¥ç¶šãƒ†ã‚¹ãƒˆ

```bash
# Pythonæ¥ç¶šãƒ†ã‚¹ãƒˆ
python << 'EOF'
import redis
import json

# æ¥ç¶š
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

# åŸºæœ¬æ“ä½œãƒ†ã‚¹ãƒˆ
r.set('test_key', 'test_value')
print(f"SETçµæœ: {r.get('test_key')}")

# JSONæ“ä½œ
test_data = {"name": "AutoForgeNexus", "type": "AI Platform"}
r.set('test_json', json.dumps(test_data))
retrieved = json.loads(r.get('test_json'))
print(f"JSONçµæœ: {retrieved}")

# å‰Šé™¤
r.delete('test_key', 'test_json')
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‰Šé™¤å®Œäº†")
EOF
```

### 2.4 Redisã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šï¼ˆæœ¬ç•ªç”¨ï¼‰

```bash
# backend/config/redis-cluster.conf
cat > backend/config/redis-cluster.conf << 'EOF'
# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®š
cluster-enabled yes
cluster-config-file nodes-6379.conf
cluster-node-timeout 5000
cluster-replica-validity-factor 10
cluster-migration-barrier 1
cluster-require-full-coverage yes

# ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
replicaof no one
replica-read-only yes
replica-serve-stale-data yes
EOF
```

---

## 3ï¸âƒ£ libSQL Vectorè¨­å®š

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

- **ä¸»æ‹…å½“**: vector-database-specialist
- **æ”¯æ´**: prompt-engineering-specialist, llm-integration

### èƒŒæ™¯ãƒ»ç›®çš„

libSQL
Vectorã¯SQLiteãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ‹¡å¼µã§ã€AIåŸ‹ã‚è¾¼ã¿ã®é«˜é€Ÿæ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚OpenAIã€Cohereã€HuggingFaceãªã©ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã—ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚„RAGï¼ˆRetrieval-Augmented
Generationï¼‰ã®åŸºç›¤ã¨ãªã‚Šã¾ã™ã€‚

### ä½¿ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ™ã‚¯ãƒˆãƒ«DBåˆæœŸåŒ–
/ai:data:vector --init --dimension 1536 --model "text-embedding-3-small"

# åŸ‹ã‚è¾¼ã¿æˆ¦ç•¥ã®è¨­è¨ˆ
/ai:prompt:create --embedding-strategy

# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®æœ€é©åŒ–
/ai:data:vector --optimize-index
```

### 3.1 Vectorãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆM1 Macå¯¾å¿œï¼‰

```bash
# M1 Macç”¨ã®ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# NumPyã®ARM64æœ€é©åŒ–ç‰ˆã‚’ç¢ºèª
pip install --upgrade pip

# ARM64ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ“ãƒ«ãƒ‰ã®NumPy
pip install numpy==1.26.4 --no-binary :all: --compile

# libSQL experimentalï¼ˆARM64ã‚µãƒãƒ¼ãƒˆç¢ºèªï¼‰
pip install libsql-experimental==0.0.30

# Sentence Transformersï¼ˆPyTorch ARM64ç‰ˆã‚’ä½¿ç”¨ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install sentence-transformers==3.3.1

# OpenAIåŸ‹ã‚è¾¼ã¿ç”¨ï¼ˆä»£æ›¿ï¼‰
pip install openai==1.55.3
pip install tiktoken==0.8.0

# ä¾å­˜é–¢ä¿‚ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç¢ºèª
pip show numpy | grep Location
file $(python -c "import numpy; print(numpy.__file__)")
# æœŸå¾…: Mach-O 64-bit ... arm64
```

### 3.2 VectoråˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰

```python
# backend/database/vector_setup.py
cat > backend/database/vector_setup.py << 'EOF'
"""libSQL VectoråˆæœŸåŒ–ã¨ãƒ†ã‚¹ãƒˆï¼ˆM1 Macæœ€é©åŒ–ï¼‰"""
import libsql_experimental as libsql
import numpy as np
import json
import os
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import logging

# OpenAIåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ï¼ˆæœ¬ç•ªæ¨å¥¨ï¼‰
try:
    from openai import OpenAI
    USE_OPENAI = True
except ImportError:
    from sentence_transformers import SentenceTransformer
    USE_OPENAI = False

load_dotenv('.env.local')

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VectorDatabase:
    def __init__(self):
        self.conn = libsql.connect(
            database=os.getenv("TURSO_DATABASE_URL"),
            auth_token=os.getenv("TURSO_AUTH_TOKEN")
        )

        if USE_OPENAI:
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.embedding_model = "text-embedding-3-small"  # 1536æ¬¡å…ƒ
            self.dimension = 1536
        else:
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            self.dimension = 384  # MiniLMã®æ¬¡å…ƒ

        logger.info(f"Vector DB initialized with {'OpenAI' if USE_OPENAI else 'SentenceTransformers'}")

    def create_collection(self, name: str, dimension: int = None):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if dimension is None:
            dimension = self.dimension

        cursor = self.conn.cursor()

        # ãƒ™ã‚¯ãƒˆãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        cursor.execute(f"""
            CREATE VIRTUAL TABLE IF NOT EXISTS {name}_vectors
            USING vec0(
                embedding FLOAT[{dimension}]
            )
        """)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS {name}_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vec_rowid INTEGER,
                content TEXT,
                metadata JSON,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        self.conn.commit()
        logger.info(f"Collection '{name}' created with dimension {dimension}")

    def insert_vector(self, collection: str, text: str, metadata: dict = None):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦æŒ¿å…¥"""
        embedding = self.model.encode(text)
        cursor = self.conn.cursor()
        cursor.execute(f"""
            INSERT INTO {collection}_vectors (embedding, metadata)
            VALUES (?, ?)
        """, (embedding.tolist(), json.dumps(metadata or {})))
        self.conn.commit()

    def search_similar(self, collection: str, query: str, k: int = 5):
        """é¡ä¼¼ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"""
        query_embedding = self.model.encode(query)
        cursor = self.conn.cursor()
        results = cursor.execute(f"""
            SELECT id, metadata, distance
            FROM {collection}_vectors
            WHERE embedding MATCH ?
            ORDER BY distance
            LIMIT ?
        """, (query_embedding.tolist(), k))
        return results.fetchall()

    def test_setup(self):
        """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
        self.create_collection("test", 384)  # MiniLMã®æ¬¡å…ƒæ•°

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
        samples = [
            "AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ",
            "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°",
            "è‡ªç„¶è¨€èªå‡¦ç†ã®å¿œç”¨"
        ]

        for text in samples:
            self.insert_vector("test", text, {"text": text})

        # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        results = self.search_similar("test", "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°", k=2)
        print("\næ¤œç´¢çµæœ:")
        for r in results:
            print(f"  - {r}")

if __name__ == "__main__":
    vdb = VectorDatabase()
    vdb.test_setup()
EOF

# å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
python backend/database/vector_setup.py
```

### 3.3 åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«è¨­å®š

```bash
# AIã‚³ãƒãƒ³ãƒ‰ã§ãƒ™ã‚¯ãƒˆãƒ«DBç®¡ç†
/ai:data:vector --init --dimension 1536 --model "text-embedding-ada-002"

# è¨­å®šç¢ºèª
/ai:data:vector --status
```

---

## 4ï¸âƒ£ SQLAlchemyãƒ»Alembicè¨­å®š

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

- **ä¸»æ‹…å½“**: backend-developer
- **æ”¯æ´**: domain-modeller, database-administrator

### èƒŒæ™¯ãƒ»ç›®çš„

SQLAlchemy 2.0ã¯Pythonã®æœ€æ–°ORMï¼ˆObject-Relational
Mappingï¼‰ã§ã€å‹å®‰å…¨æ€§ã¨éåŒæœŸã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚Alembicã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨å®‰å…¨ãªãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### ä½¿ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆ
/ai:requirements:domain --entity-mapping

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥
/ai:data:migrate --strategy-design
```

### 4.1 SQLAlchemyè¨­å®š

```python
# backend/database/base.py
cat > backend/database/base.py << 'EOF'
"""ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŸºæœ¬è¨­å®š"""
from sqlalchemy import create_engine, event
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import QueuePool
import os
from dotenv import load_dotenv

load_dotenv('.env.local')

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLæ§‹ç¯‰
DATABASE_URL = os.getenv("TURSO_DATABASE_URL")
AUTH_TOKEN = os.getenv("TURSO_AUTH_TOKEN")

# æ¥ç¶šURLä½œæˆï¼ˆlibSQLç”¨ï¼‰
if DATABASE_URL and AUTH_TOKEN:
    connection_url = f"{DATABASE_URL}?authToken={AUTH_TOKEN}"
else:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«SQLiteï¼‰
    connection_url = "sqlite:///./backend/data/local.db"

# ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
engine = create_engine(
    connection_url,
    poolclass=QueuePool,
    pool_size=int(os.getenv("DB_POOL_SIZE", 20)),
    max_overflow=int(os.getenv("DB_MAX_OVERFLOW", 40)),
    pool_timeout=int(os.getenv("DB_POOL_TIMEOUT", 30)),
    pool_recycle=int(os.getenv("DB_POOL_RECYCLE", 3600)),
    echo=os.getenv("DB_ECHO", "false").lower() == "true",
    connect_args={
        "check_same_thread": False,  # SQLiteç”¨
        "timeout": 30
    } if "sqlite" in connection_url else {}
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine
)

# ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹
Base = declarative_base()

# WALãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆSQLite/libSQLç”¨ï¼‰
if "sqlite" in connection_url or "libsql" in connection_url:
    @event.listens_for(engine, "connect")
    def set_sqlite_pragma(dbapi_conn, connection_record):
        cursor = dbapi_conn.cursor()
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL")
        cursor.execute("PRAGMA cache_size=10000")
        cursor.execute("PRAGMA temp_store=MEMORY")
        cursor.close()

def get_db() -> Session:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# ãƒ†ã‚¹ãƒˆæ¥ç¶š
if __name__ == "__main__":
    with SessionLocal() as session:
        result = session.execute("SELECT 1")
        print(f"Database connection test: {result.scalar()}")
EOF

# æ¥ç¶šãƒ†ã‚¹ãƒˆ
python backend/database/base.py
```

### 4.2 AlembicåˆæœŸåŒ–

```bash
# AlembicåˆæœŸåŒ–
cd backend
alembic init migrations

# alembic.iniè¨­å®š
cat > alembic.ini << 'EOF'
[alembic]
script_location = migrations
prepend_sys_path = .
version_path_separator = os
sqlalchemy.url = driver://user:pass@localhost/dbname  # env.pyã§ä¸Šæ›¸ã

[post_write_hooks]
hooks = black
black.type = console_scripts
black.entrypoint = black
black.options = -l 88

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
EOF
```

### 4.3 Alembicç’°å¢ƒè¨­å®š

```python
# backend/migrations/env.py
cat > backend/migrations/env.py << 'EOF'
"""Alembicç’°å¢ƒè¨­å®š"""
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from database.base import Base, connection_url
from models import *  # ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

config = context.config
fileConfig(config.config_file_name)

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š
target_metadata = Base.metadata

# æ¥ç¶šURLè¨­å®š
config.set_main_option("sqlalchemy.url", connection_url)

def run_migrations_offline():
    """ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
EOF
```

### 4.4 åˆæœŸãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ

```bash
# åˆæœŸãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
alembic revision --autogenerate -m "Initial schema"

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
alembic upgrade head

# ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
alembic current
```

---

## 5ï¸âƒ£ Dockerçµ±åˆ

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

- **ä¸»æ‹…å½“**: devops-coordinator
- **æ”¯æ´**: sre-agent-agent, edge-computing-specialist

### èƒŒæ™¯ãƒ»ç›®çš„

Dockerã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã«ã‚ˆã‚Šã€é–‹ç™ºç’°å¢ƒã®ä¸€è²«æ€§ã¨æœ¬ç•ªç’°å¢ƒã¸ã®ç§»è¡Œå¯èƒ½æ€§ã‚’ä¿è¨¼ã—ã¾ã™ã€‚docker-composeã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚³ãƒ³ãƒ†ãƒŠç®¡ç†ã§ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒ—ãƒ­ã‚­ã‚·ã‚’çµ±åˆçš„ã«ç®¡ç†ã—ã¾ã™ã€‚

### ä½¿ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# Dockerç’°å¢ƒã®æ§‹ç¯‰
/ai:operations:deploy --docker-setup

# ã‚³ãƒ³ãƒ†ãƒŠæœ€é©åŒ–
/sc:build --optimize-containers
```

### 5.1 Docker Composeè¨­å®šï¼ˆM1 Macå¯¾å¿œï¼‰

```yaml
# docker-compose.database.yml
cat > docker-compose.database.yml << 'EOF'
version: '3.9'

services:
  # Redis Cache (ARM64å¯¾å¿œ)
  redis:
    image: redis:7.4.1-alpine
    platform: linux/arm64  # M1 Macç”¨ã«æ˜ç¤ºçš„ã«æŒ‡å®š
    container_name: autoforge_redis
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis:/data
      - ./backend/config/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - autoforge_network
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}

  # Redis Commander (ç®¡ç†UI - ARM64å¯¾å¿œ)
  redis-commander:
    image: rediscommander/redis-commander:latest
    platform: linux/arm64
    container_name: autoforge_redis_commander
    environment:
      - REDIS_HOSTS=local:redis:6379:0:${REDIS_PASSWORD}
      - HTTP_USER=admin
      - HTTP_PASSWORD=${REDIS_COMMANDER_PASSWORD:-admin}
    ports:
      - "8081:8081"
    networks:
      - autoforge_network
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped

  # Turso Edge Proxy (ARM64ãƒã‚¤ãƒ†ã‚£ãƒ–)
  turso-proxy:
    image: ghcr.io/tursodatabase/libsql-server:latest
    platform: linux/arm64
    container_name: autoforge_turso_proxy
    ports:
      - "8080:8080"
      - "5001:5001"
    environment:
      - SQLD_DB_PATH=/var/lib/sqld/data.db
      - SQLD_HTTP_LISTEN_ADDR=0.0.0.0:8080
      - SQLD_GRPC_LISTEN_ADDR=0.0.0.0:5001
      - SQLD_AUTH_JWT_KEY_FILE=/var/lib/sqld/jwt.key  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
    volumes:
      - ./data/turso:/var/lib/sqld
      - ./backend/config/jwt.key:/var/lib/sqld/jwt.key:ro
    networks:
      - autoforge_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G

networks:
  autoforge_network:
    driver: bridge

volumes:
  redis_data:
  turso_data:
EOF

# èµ·å‹•
docker-compose -f docker-compose.database.yml up -d

# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
docker-compose -f docker-compose.database.yml ps

# ãƒ­ã‚°ç¢ºèª
docker-compose -f docker-compose.database.yml logs -f
```

### 5.2 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# backend/scripts/init_db.sh
cat > backend/scripts/init_db.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–é–‹å§‹..."

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
source backend/.env.local

# Tursoæ¥ç¶šç¢ºèª
echo "ğŸ“¡ Tursoæ¥ç¶šç¢ºèª..."
turso db shell $TURSO_DATABASE_NAME "SELECT 1" || {
    echo "âŒ Tursoæ¥ç¶šå¤±æ•—"
    exit 1
}

# Redisæ¥ç¶šç¢ºèª
echo "ğŸ“¡ Redisæ¥ç¶šç¢ºèª..."
redis-cli ping || {
    echo "âŒ Redisæ¥ç¶šå¤±æ•—"
    exit 1
}

# ã‚¹ã‚­ãƒ¼ãƒé©ç”¨
echo "ğŸ“‹ ã‚¹ã‚­ãƒ¼ãƒé©ç”¨..."
turso db shell $TURSO_DATABASE_NAME < backend/sql/001_initial_schema.sql

# Alembicãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
echo "ğŸ”„ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ..."
cd backend
alembic upgrade head
cd ..

# ãƒ™ã‚¯ãƒˆãƒ«DBåˆæœŸåŒ–
echo "ğŸ¯ ãƒ™ã‚¯ãƒˆãƒ«DBåˆæœŸåŒ–..."
python backend/database/vector_setup.py

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
if [ "$ENVIRONMENT" = "development" ]; then
    echo "ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥..."
    python backend/scripts/seed_data.py
fi

echo "âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†!"
EOF

chmod +x backend/scripts/init_db.sh

# å®Ÿè¡Œ
./backend/scripts/init_db.sh
```

---

## 6ï¸âƒ£ å‹•ä½œç¢ºèªã¨ãƒ†ã‚¹ãƒˆ

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

- **ä¸»æ‹…å½“**: test-automation-engineer
- **æ”¯æ´**: qa-coordinator, performance-optimizer

### èƒŒæ™¯ãƒ»ç›®çš„

åŒ…æ‹¬çš„ãªçµ±åˆãƒ†ã‚¹ãƒˆã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å±¤ã®ä¿¡é ¼æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã¯ã€ç¶™ç¶šçš„ãªå“è³ªä¿è¨¼ã®åŸºç›¤ã¨ãªã‚Šã¾ã™ã€‚

### ä½¿ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¨å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
/sc:test --integration --coverage

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
/ai:operations:monitor --performance-test

# ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼
/ai:data:analyze --integrity-check
```

### 6.1 çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
# backend/tests/test_database_integration.py
cat > backend/tests/test_database_integration.py << 'EOF'
"""ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆãƒ†ã‚¹ãƒˆ"""
import pytest
import asyncio
import redis
import libsql_experimental as libsql
from sqlalchemy import create_engine, text
import numpy as np
from sentence_transformers import SentenceTransformer
import time
import json
import os
from dotenv import load_dotenv

load_dotenv('.env.local')

class DatabaseIntegrationTest:
    def __init__(self):
        self.results = []
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

    def test_turso_connection(self):
        """Tursoæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            conn = libsql.connect(
                database=os.getenv("TURSO_DATABASE_URL"),
                auth_token=os.getenv("TURSO_AUTH_TOKEN")
            )
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            assert result[0] == 1
            self.results.append("âœ… Tursoæ¥ç¶š: æˆåŠŸ")
        except Exception as e:
            self.results.append(f"âŒ Tursoæ¥ç¶š: {e}")

    def test_redis_operations(self):
        """Redisæ“ä½œãƒ†ã‚¹ãƒˆ"""
        try:
            r = redis.Redis(host='localhost', port=6379, decode_responses=True)

            # åŸºæœ¬æ“ä½œ
            r.set('test:key', 'test_value', ex=60)
            assert r.get('test:key') == 'test_value'

            # JSONæ“ä½œ
            data = {"id": 1, "name": "test"}
            r.set('test:json', json.dumps(data))
            retrieved = json.loads(r.get('test:json'))
            assert retrieved == data

            # å‰Šé™¤
            r.delete('test:key', 'test:json')

            self.results.append("âœ… Redisæ“ä½œ: æˆåŠŸ")
        except Exception as e:
            self.results.append(f"âŒ Redisæ“ä½œ: {e}")

    def test_vector_operations(self):
        """ãƒ™ã‚¯ãƒˆãƒ«æ“ä½œãƒ†ã‚¹ãƒˆ"""
        try:
            conn = libsql.connect(
                database=os.getenv("TURSO_DATABASE_URL"),
                auth_token=os.getenv("TURSO_AUTH_TOKEN")
            )
            cursor = conn.cursor()

            # ãƒ™ã‚¯ãƒˆãƒ«æŒ¿å…¥
            text = "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
            embedding = self.model.encode(text)

            cursor.execute("""
                INSERT INTO test_vectors (embedding, metadata)
                VALUES (?, ?)
            """, (embedding.tolist(), json.dumps({"text": text})))

            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
            query = "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–"
            query_embedding = self.model.encode(query)

            results = cursor.execute("""
                SELECT metadata, distance
                FROM test_vectors
                WHERE embedding MATCH ?
                ORDER BY distance
                LIMIT 3
            """, (query_embedding.tolist(),))

            assert results.fetchone() is not None
            self.results.append("âœ… ãƒ™ã‚¯ãƒˆãƒ«æ“ä½œ: æˆåŠŸ")
        except Exception as e:
            self.results.append(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«æ“ä½œ: {e}")

    def test_sqlalchemy_orm(self):
        """SQLAlchemy ORM ãƒ†ã‚¹ãƒˆ"""
        try:
            from database.base import SessionLocal

            with SessionLocal() as session:
                # ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
                result = session.execute(text("SELECT COUNT(*) FROM users"))
                count = result.scalar()
                assert count is not None

                self.results.append("âœ… SQLAlchemy ORM: æˆåŠŸ")
        except Exception as e:
            self.results.append(f"âŒ SQLAlchemy ORM: {e}")

    def test_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            conn = libsql.connect(
                database=os.getenv("TURSO_DATABASE_URL"),
                auth_token=os.getenv("TURSO_AUTH_TOKEN")
            )
            cursor = conn.cursor()

            # æ›¸ãè¾¼ã¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            start = time.time()
            for i in range(100):
                cursor.execute(
                    "INSERT INTO test_perf (data) VALUES (?)",
                    (f"test_data_{i}",)
                )
            conn.commit()
            write_time = time.time() - start

            # èª­ã¿å–ã‚Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            start = time.time()
            cursor.execute("SELECT * FROM test_perf LIMIT 100")
            cursor.fetchall()
            read_time = time.time() - start

            self.results.append(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: æ›¸è¾¼{write_time:.2f}s, èª­å–{read_time:.2f}s")

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cursor.execute("DROP TABLE IF EXISTS test_perf")
            conn.commit()
        except Exception as e:
            self.results.append(f"âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {e}")

    def run_all_tests(self):
        """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("\nğŸ”¬ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹\n")
        print("-" * 50)

        self.test_turso_connection()
        self.test_redis_operations()
        self.test_vector_operations()
        self.test_sqlalchemy_orm()
        self.test_performance()

        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ:")
        print("-" * 50)
        for result in self.results:
            print(result)

        # æˆåŠŸ/å¤±æ•—ã‚«ã‚¦ãƒ³ãƒˆ
        success = len([r for r in self.results if "âœ…" in r])
        failed = len([r for r in self.results if "âŒ" in r])

        print("-" * 50)
        print(f"\nç·è¨ˆ: æˆåŠŸ {success}/{len(self.results)}, å¤±æ•— {failed}/{len(self.results)}")

        return failed == 0

if __name__ == "__main__":
    test = DatabaseIntegrationTest()
    success = test.run_all_tests()
    exit(0 if success else 1)
EOF

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python backend/tests/test_database_integration.py
```

### 6.2 ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

```python
# backend/api/health.py
cat > backend/api/health.py << 'EOF'
"""ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
from fastapi import APIRouter, HTTPException
import redis
import libsql_experimental as libsql
from database.base import SessionLocal
import os
import time

router = APIRouter(prefix="/health", tags=["health"])

@router.get("/database")
async def check_database_health():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
    health_status = {
        "turso": False,
        "redis": False,
        "vector": False,
        "timestamp": time.time()
    }

    # Tursoç¢ºèª
    try:
        with SessionLocal() as session:
            session.execute("SELECT 1")
            health_status["turso"] = True
    except Exception as e:
        health_status["turso_error"] = str(e)

    # Redisç¢ºèª
    try:
        r = redis.Redis(host='localhost', port=6379)
        r.ping()
        health_status["redis"] = True
    except Exception as e:
        health_status["redis_error"] = str(e)

    # Vector DBç¢ºèª
    try:
        conn = libsql.connect(
            database=os.getenv("TURSO_DATABASE_URL"),
            auth_token=os.getenv("TURSO_AUTH_TOKEN")
        )
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE name LIKE 'vec_%'")
        if cursor.fetchone():
            health_status["vector"] = True
    except Exception as e:
        health_status["vector_error"] = str(e)

    # ç·åˆåˆ¤å®š
    all_healthy = all([
        health_status["turso"],
        health_status["redis"],
        health_status["vector"]
    ])

    if not all_healthy:
        raise HTTPException(status_code=503, detail=health_status)

    return health_status
EOF
```

---

## 7ï¸âƒ£ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

- **ä¸»æ‹…å½“**: root-cause-analyst
- **æ”¯æ´**: performance-engineer, sre-agent-agent

### ä½¿ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# å•é¡Œè¨ºæ–­
/ai:troubleshoot --database --deep-analysis

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
/ai:operations:monitor --bottleneck-detection

# ãƒ­ã‚°åˆ†æ
/sc:analyze --logs --pattern-detection
```

### M1 Macå›ºæœ‰ã®å•é¡Œã¨è§£æ±ºç­–

#### Docker ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ

```bash
# ã‚¨ãƒ©ãƒ¼: DockerãŒé…ã„ã€CPUä½¿ç”¨ç‡ãŒé«˜ã„
# è§£æ±ºç­–:

# Docker Desktopè¨­å®šæœ€é©åŒ–
cat > ~/.docker/daemon.json << 'EOF'
{
  "experimental": true,
  "features": {
    "buildkit": true
  },
  "builder": {
    "gc": {
      "defaultKeepStorage": "20GB",
      "enabled": true
    }
  },
  "cpu-rt-runtime": 95000,
  "cpu-rt-period": 100000
}
EOF

# Docker Desktopå†èµ·å‹•
osascript -e 'quit app "Docker"'
sleep 5
open -a Docker
```

#### Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ARM64äº’æ›æ€§

```bash
# ã‚¨ãƒ©ãƒ¼: ImportError: dlopen failed
# è§£æ±ºç­–:

# Condaç’°å¢ƒã‚’ä½¿ç”¨ï¼ˆARM64ãƒã‚¤ãƒ†ã‚£ãƒ–ï¼‰
conda create -n autoforge python=3.13
conda activate autoforge
conda install -c conda-forge numpy scipy

# ã¾ãŸã¯ã€Rosetta 2çµŒç”±ã§x86_64ç‰ˆã‚’å®Ÿè¡Œ
arch -x86_64 pip install numpy
```

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

#### Tursoæ¥ç¶šã‚¨ãƒ©ãƒ¼

```bash
# ã‚¨ãƒ©ãƒ¼: Connection refused
# è§£æ±ºç­–:
turso auth login  # å†èªè¨¼
turso db show autoforgenexus-dev  # DBå­˜åœ¨ç¢ºèª
curl -I https://api.turso.tech  # APIåˆ°é”ç¢ºèª

# M1 Macã§ã®ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ç¢ºèª
sudo /usr/libexec/ApplicationFirewall/socketfilterfw --getglobalstate
```

#### Redisæ¥ç¶šã‚¨ãƒ©ãƒ¼

```bash
# ã‚¨ãƒ©ãƒ¼: Could not connect to Redis
# è§£æ±ºç­–:
redis-cli ping  # Redisèµ·å‹•ç¢ºèª
ps aux | grep redis  # ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
sudo systemctl restart redis-server  # å†èµ·å‹•

# è¨­å®šç¢ºèª
redis-cli CONFIG GET bind
redis-cli CONFIG GET protected-mode
```

#### ãƒ™ã‚¯ãƒˆãƒ«æ“ä½œã‚¨ãƒ©ãƒ¼

```bash
# ã‚¨ãƒ©ãƒ¼: Vector extension not found
# è§£æ±ºç­–:
turso db shell autoforgenexus-dev << 'EOF'
-- æ‹¡å¼µå†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
DROP TABLE IF EXISTS vec_items;
CREATE VIRTUAL TABLE vec_items
USING vec0(embedding float[1536]);
EOF

# Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip uninstall libsql-experimental -y
pip install libsql-experimental==0.0.30
```

#### ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼

```bash
# ã‚¨ãƒ©ãƒ¼: Alembic migration failed
# è§£æ±ºç­–:
alembic downgrade -1  # 1ã¤å‰ã«æˆ»ã™
alembic history  # å±¥æ­´ç¢ºèª
alembic current  # ç¾åœ¨ä½ç½®ç¢ºèª
alembic upgrade head  # å†å®Ÿè¡Œ

# ãƒªã‚»ãƒƒãƒˆï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
rm -rf backend/migrations/versions/*
alembic revision --autogenerate -m "Reset"
alembic upgrade head
```

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œï¼ˆM1 Macæœ€é©åŒ–ï¼‰

```bash
# Tursoæœ€é©åŒ–ï¼ˆM1å‘ã‘ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
turso db shell autoforgenexus-dev << 'EOF'
PRAGMA cache_size = 20000;  # M1ã®ãƒ¡ãƒ¢ãƒªå¸¯åŸŸã‚’æ´»ç”¨
PRAGMA temp_store = MEMORY;
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA mmap_size = 30000000000;  # 30GB mmapï¼ˆM1ã®é«˜é€ŸSSDã‚’æ´»ç”¨ï¼‰
PRAGMA page_size = 8192;  # ã‚ˆã‚Šå¤§ããªãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º
ANALYZE;  -- çµ±è¨ˆæ›´æ–°
EOF

# Redisæœ€é©åŒ–ï¼ˆARM64å‘ã‘ï¼‰
redis-cli << 'EOF'
CONFIG SET maxmemory 4gb
CONFIG SET maxmemory-policy allkeys-lru
CONFIG SET io-threads 4  # M1ã®ãƒãƒ«ãƒã‚³ã‚¢æ´»ç”¨
CONFIG SET io-threads-do-reads yes
CONFIG SET save ""  # æ°¸ç¶šåŒ–ç„¡åŠ¹ï¼ˆé«˜é€ŸåŒ–ï¼‰
EOF

# ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ï¼ˆmacOSï¼‰
sudo sysctl -w kern.maxfiles=65536
sudo sysctl -w kern.maxfilesperproc=65536
sudo sysctl -w net.inet.tcp.msl=1000
```

### è¨ºæ–­ã‚³ãƒãƒ³ãƒ‰é›†ï¼ˆM1 Macå¯¾å¿œï¼‰

```bash
# ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèªï¼ˆmacOSï¼‰
sudo powermetrics --samplers cpu_power,gpu_power -i 1000 -n 1  # M1é›»åŠ›ä½¿ç”¨çŠ¶æ³
top -o cpu  # CPUä½¿ç”¨ç‡é †
sudo fs_usage -w | grep -E 'turso|redis'  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
netstat -an | grep -E '6379|8080'  # ãƒãƒ¼ãƒˆç¢ºèª

# M1 å›ºæœ‰ã®ãƒ¡ãƒ¢ãƒªåœ§åŠ›ç¢ºèª
vm_stat 1  # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ
memory_pressure  # ãƒ¡ãƒ¢ãƒªåœ§åŠ›ãƒ†ã‚¹ãƒˆ

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºç¢ºèª
turso db show autoforgenexus-dev --size
redis-cli INFO memory | grep used_memory_human

# Docker ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
docker stats --no-stream
docker system df

# ãƒ­ã‚°ç¢ºèª
tail -f backend/logs/database.log
docker-compose -f docker-compose.database.yml logs -f
```

---

## 8ï¸âƒ£ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

- **ä¸»æ‹…å½“**: security-architect
- **æ”¯æ´**: compliance-officer, sre-agent-agent

### ä½¿ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
/ai:quality:security --audit --database

# ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
/ai:requirements:define --security-requirements
```

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼ˆå¼·åŒ–ç‰ˆï¼‰

```bash
# 1. ç’°å¢ƒå¤‰æ•°æš—å·åŒ–ï¼ˆmacOS Keychainçµ±åˆï¼‰
# backend/utils/secure_env.py
cat > backend/utils/secure_env.py << 'EOF'
"""ã‚»ã‚­ãƒ¥ã‚¢ç’°å¢ƒå¤‰æ•°ç®¡ç†ï¼ˆM1 Macå¯¾å¿œï¼‰"""
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
import os
import subprocess
import base64
import json
from pathlib import Path

class SecureEnvManager:
    def __init__(self):
        self.keychain_service = "AutoForgeNexus"
        self.keychain_account = "env_encryption"

    def generate_key(self, password: str) -> bytes:
        """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼ç”Ÿæˆ"""
        kdf = PBKDF2(
            algorithm=hashes.SHA256(),
            length=32,
            salt=b'autoforge_salt_v1',
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(password.encode()))
        return key

    def store_in_keychain(self, key: str):
        """macOS Keychainã«ã‚­ãƒ¼ã‚’ä¿å­˜"""
        subprocess.run([
            "security", "add-generic-password",
            "-a", self.keychain_account,
            "-s", self.keychain_service,
            "-w", key,
            "-U"  # æ—¢å­˜ã‚’æ›´æ–°
        ])

    def get_from_keychain(self) -> str:
        """macOS Keychainã‹ã‚‰ã‚­ãƒ¼ã‚’å–å¾—"""
        result = subprocess.run([
            "security", "find-generic-password",
            "-a", self.keychain_account,
            "-s", self.keychain_service,
            "-w"
        ], capture_output=True, text=True)
        return result.stdout.strip()

    def encrypt_env_file(self):
        """ç’°å¢ƒå¤‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«æš—å·åŒ–"""
        # ãƒã‚¹ã‚¿ãƒ¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å–å¾—ï¼ˆã¾ãŸã¯ç”Ÿæˆï¼‰
        import getpass
        password = getpass.getpass("Enter master password: ")

        key = self.generate_key(password)
        cipher = Fernet(key)

        # æš—å·åŒ–
        with open('.env.local', 'rb') as f:
            encrypted = cipher.encrypt(f.read())

        # ä¿å­˜
        with open('.env.local.enc', 'wb') as f:
            f.write(encrypted)

        # Keychainã«ã‚­ãƒ¼ä¿å­˜
        self.store_in_keychain(key.decode())

        print("âœ… Environment variables encrypted and key stored in macOS Keychain")

if __name__ == "__main__":
    manager = SecureEnvManager()
    manager.encrypt_env_file()
EOF

# 2. æ¥ç¶šåˆ¶é™
turso db update autoforgenexus-prod \
  --allowed-locations nrt,sin  # ç‰¹å®šãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã®ã¿

# 3. ç›£æŸ»ãƒ­ã‚°
turso db audit autoforgenexus-prod --enable
```

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

```bash
# backend/scripts/backup_db.sh
cat > backend/scripts/backup_db.sh << 'EOF'
#!/bin/bash
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Tursoãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
turso db dump autoforgenexus-prod > backups/turso_$TIMESTAMP.sql

# Redisãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
redis-cli BGSAVE
cp data/redis/dump.rdb backups/redis_$TIMESTAMP.rdb

# S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆæœ¬ç•ªï¼‰
aws s3 cp backups/ s3://autoforge-backups/$TIMESTAMP/ --recursive

echo "âœ… Backup completed: $TIMESTAMP"
EOF

# Cronè¨­å®šï¼ˆæ¯æ—¥2AMï¼‰
echo "0 2 * * * /path/to/backup_db.sh" | crontab -
```

### ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®šï¼ˆåŒ…æ‹¬çš„ï¼‰

```python
# backend/monitoring/db_metrics.py
cat > backend/monitoring/db_metrics.py << 'EOF'
"""ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ï¼ˆæœ¬ç•ªå¯¾å¿œï¼‰"""
import prometheus_client as prom
import redis
import libsql_experimental as libsql
import psutil
import time
import json
import logging
from typing import Dict, Any
from datetime import datetime
import asyncio

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
db_connections = prom.Gauge('db_connections', 'Active DB connections', ['database'])
cache_hit_rate = prom.Gauge('cache_hit_rate', 'Redis cache hit rate')
query_duration = prom.Histogram('query_duration_seconds', 'Query execution time', ['operation'])
vector_search_latency = prom.Histogram('vector_search_latency_seconds', 'Vector search latency')
memory_usage = prom.Gauge('memory_usage_bytes', 'Memory usage', ['type'])
cpu_usage = prom.Gauge('cpu_usage_percent', 'CPU usage')

# ã‚¢ãƒ©ãƒ¼ãƒˆé—¾å€¤
ALERT_THRESHOLDS = {
    'cpu_high': 80,
    'memory_high': 85,
    'cache_hit_low': 0.7,
    'query_slow': 1.0,  # ç§’
    'connection_limit': 100
}

class DatabaseMonitor:
    def __init__(self):
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            password=os.getenv('REDIS_PASSWORD'),
            decode_responses=True
        )
        self.alerts = []

    async def collect_metrics(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        while True:
            try:
                # CPU/ãƒ¡ãƒ¢ãƒª
                cpu_percent = psutil.cpu_percent(interval=1)
                cpu_usage.set(cpu_percent)

                mem = psutil.virtual_memory()
                memory_usage.labels(type='used').set(mem.used)
                memory_usage.labels(type='available').set(mem.available)

                # Redisãƒ¡ãƒˆãƒªã‚¯ã‚¹
                info = self.redis_client.info('stats')
                if info['keyspace_hits'] + info['keyspace_misses'] > 0:
                    hit_rate = info['keyspace_hits'] / (
                        info['keyspace_hits'] + info['keyspace_misses']
                    )
                    cache_hit_rate.set(hit_rate)

                    # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
                    if hit_rate < ALERT_THRESHOLDS['cache_hit_low']:
                        self.trigger_alert('LOW_CACHE_HIT_RATE', hit_rate)

                # ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆ
                if cpu_percent > ALERT_THRESHOLDS['cpu_high']:
                    self.trigger_alert('HIGH_CPU_USAGE', cpu_percent)

                if mem.percent > ALERT_THRESHOLDS['memory_high']:
                    self.trigger_alert('HIGH_MEMORY_USAGE', mem.percent)

            except Exception as e:
                logger.error(f"Metrics collection error: {e}")

            await asyncio.sleep(10)  # 10ç§’ã”ã¨ã«åé›†

    def trigger_alert(self, alert_type: str, value: Any):
        """ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«"""
        alert = {
            'timestamp': datetime.utcnow().isoformat(),
            'type': alert_type,
            'value': value,
            'threshold': ALERT_THRESHOLDS.get(alert_type.lower().replace('_', ''), 'N/A')
        }

        self.alerts.append(alert)
        logger.warning(f"ALERT: {alert}")

        # Slack/Discordé€šçŸ¥ï¼ˆå®Ÿè£…ä¾‹ï¼‰
        # self.send_notification(alert)

    async def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        # Prometheusã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆèµ·å‹•
        prom.start_http_server(9090)
        logger.info("Monitoring started on http://localhost:9090/metrics")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†é–‹å§‹
        await self.collect_metrics()

if __name__ == "__main__":
    monitor = DatabaseMonitor()
    asyncio.run(monitor.start_monitoring())
EOF

def collect_metrics():
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
    # æ¥ç¶šæ•°
    # ... å®Ÿè£…

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡
    r = redis.Redis()
    info = r.info('stats')
    hit_rate = info['keyspace_hits'] / (info['keyspace_hits'] + info['keyspace_misses'])
    cache_hit_rate.set(hit_rate)

    # Prometheusã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆèµ·å‹•
    prom.start_http_server(8000)
EOF
```

### é–‹ç™ºã®ã‚³ãƒ„

```yaml
æ¨å¥¨äº‹é …:
  - ãƒ–ãƒ©ãƒ³ãƒæˆ¦ç•¥: main/dev/feature-*ã§TursoDBåˆ†é›¢
  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥: é »ç¹ã‚¢ã‚¯ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯å¿…ãšRedisçµŒç”±
  - ãƒ™ã‚¯ãƒˆãƒ«æœ€é©åŒ–: ãƒãƒƒãƒå‡¦ç†ã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
  - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: ã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå¾Œã«è¿½åŠ 
  - æ¥ç¶šãƒ—ãƒ¼ãƒ«: æœ¬ç•ªã¯å¿…ãšæ¥ç¶šãƒ—ãƒ¼ãƒ«ä½¿ç”¨

é¿ã‘ã‚‹ã¹ãã“ã¨:
  - N+1ã‚¯ã‚¨ãƒªå•é¡Œ
  - åŒæœŸçš„ãªå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®ç¹°ã‚Šè¿”ã—ã‚¯ã‚¨ãƒª
  - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã—ã®ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³
  - ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å¤–éƒ¨APIå‘¼ã³å‡ºã—
```

---

## âœ… å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å¿…é ˆé …ç›®

- [ ] Turso DBã®3ç’°å¢ƒæ§‹ç¯‰å®Œäº†ï¼ˆdev/staging/prodï¼‰
- [ ] Redisèµ·å‹•ã¨ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æˆåŠŸ
- [ ] libSQL Vectoræ‹¡å¼µæœ‰åŠ¹åŒ–
- [ ] SQLAlchemyæ¥ç¶šç¢ºèª
- [ ] AlembicåˆæœŸãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
- [ ] Docker Composeçµ±åˆå®Œäº†
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆå…¨é …ç›®ãƒ‘ã‚¹
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¨­å®š
- [ ] ç’°å¢ƒå¤‰æ•°ã™ã¹ã¦è¨­å®šæ¸ˆã¿

### ç¢ºèªã‚³ãƒãƒ³ãƒ‰

```bash
# æœ€çµ‚ç¢ºèª
/ai:data:analyze --health-check
python backend/tests/test_database_integration.py
curl http://localhost:8000/health/database

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª
/ai:operations:monitor --database-metrics
```

---

## ğŸ¯ Phase 5ã¸ã®æ¥ç¶š

ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç’°å¢ƒãŒæ§‹ç¯‰ã§ããŸã‚‰ã€æ¬¡ã¯[Phase 5: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç’°å¢ƒæ§‹ç¯‰](./PHASE5_FRONTEND_ENVIRONMENT_SETUP.md)ã¸é€²ã‚“ã§ãã ã•ã„ã€‚

### Phase 5ã§ä½¿ç”¨ã™ã‚‹æƒ…å ±

```bash
# ä»¥ä¸‹ã®æƒ…å ±ã‚’Phase 5ã§ä½¿ç”¨
export NEXT_PUBLIC_DATABASE_URL=$TURSO_DATABASE_URL
export NEXT_PUBLIC_REDIS_URL=$REDIS_URL
export NEXT_PUBLIC_VECTOR_API_ENDPOINT="http://localhost:8000/api/v1/vectors"
```

### çµ±åˆãƒã‚¤ãƒ³ãƒˆ

- APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: `http://localhost:8000`
- WebSocket: `ws://localhost:8000/ws`
- Redis Pub/Sub: `redis://localhost:6379`

---

## ğŸ“š å‚è€ƒè³‡æ–™

- [Tursoå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.turso.tech)
- [libSQL Vector Guide](https://github.com/tursodatabase/libsql/blob/main/docs/vector.md)
- [Redis Best Practices](https://redis.io/docs/manual/patterns/)
- [SQLAlchemy 2.0 Documentation](https://docs.sqlalchemy.org/en/20/)
- [Alembic Tutorial](https://alembic.sqlalchemy.org/en/latest/tutorial.html)

## ğŸ†˜ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆï¼š

1. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#7ï¸âƒ£-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç¢ºèª
2. `backend/logs/`ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
3. AIã‚³ãƒãƒ³ãƒ‰ä½¿ç”¨: `/ai:troubleshoot --database`
4. GitHub Issuesã§å ±å‘Š

---

_Last Updated: 2024-12-24_ _Version: 1.0.0_
