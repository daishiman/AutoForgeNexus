# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Ÿè£…ã‚¬ã‚¤ãƒ‰

**å®Ÿè£…å¯¾è±¡**: AutoForgeNexus FastAPIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
**ç›®æ¨™**: P95 < 200ms, WebSocket 10,000+æ¥ç¶š, ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–

## ğŸ”§ Critical Priority å®Ÿè£…

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«å®Ÿè£…

**å•é¡Œ**: ç¾åœ¨ã®settings.pyã§è¨­å®šã¯å®šç¾©ã•ã‚Œã¦ã„ã‚‹ãŒã€å®Ÿéš›ã®æ¥ç¶šãƒ—ãƒ¼ãƒ«å®Ÿè£…ãŒä¸å®Œå…¨

**å®Ÿè£…ä¾‹**:

```python
# src/infrastructure/shared/database/connection.py
from typing import AsyncGenerator
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.pool import QueuePool
from ...core.config.settings import Settings

settings = Settings()

# é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
DATABASE_CONFIG = {
    "pool_size": 20,                    # åŸºæœ¬æ¥ç¶šæ•°
    "max_overflow": 15,                 # è¿½åŠ æ¥ç¶šè¨±å¯
    "pool_timeout": 10,                 # æ¥ç¶šå–å¾—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    "pool_recycle": 3600,              # 1æ™‚é–“ã§æ¥ç¶šå†ç”Ÿæˆ
    "pool_pre_ping": True,             # æ¥ç¶šå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
}

# éåŒæœŸã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
async_engine = create_async_engine(
    settings.get_database_url(),
    **DATABASE_CONFIG,
    echo=settings.database_echo,
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¯ãƒˆãƒª
AsyncSessionLocal = async_sessionmaker(
    bind=async_engine,
    class_=AsyncSession,
    expire_on_commit=False,
)

async def get_database_session() -> AsyncGenerator[AsyncSession, None]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—ï¼ˆä¾å­˜æ€§æ³¨å…¥ç”¨ï¼‰"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

# æ¥ç¶šãƒ—ãƒ¼ãƒ«ç›£è¦–æ©Ÿèƒ½
async def get_pool_status() -> dict:
    """æ¥ç¶šãƒ—ãƒ¼ãƒ«çŠ¶æ…‹å–å¾—"""
    pool = async_engine.pool
    return {
        "pool_size": pool.size(),
        "checked_in": pool.checkedin(),
        "checked_out": pool.checkedout(),
        "overflow": pool.overflow(),
        "invalid": pool.invalid()
    }
```

### 2. Redis æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–

**å®Ÿè£…ä¾‹**:

```python
# src/infrastructure/shared/cache/redis_client.py
import redis.asyncio as redis
from typing import Optional, Any
import json
import pickle
from ...core.config.settings import Settings

settings = Settings()

class OptimizedRedisClient:
    def __init__(self):
        self.pool = None
        self.client = None

    async def initialize(self):
        """Redisæ¥ç¶šãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–"""
        self.pool = redis.ConnectionPool.from_url(
            settings.get_redis_url(),
            max_connections=50,          # é«˜ã„åŒæ™‚æ¥ç¶šæ•°
            retry_on_timeout=True,
            decode_responses=False,       # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿å¯¾å¿œ
            socket_keepalive=True,
            socket_keepalive_options={},
        )
        self.client = redis.Redis(connection_pool=self.pool)

    async def get_with_fallback(self, key: str, fallback_func=None) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            cached = await self.client.get(key)
            if cached:
                return pickle.loads(cached)
        except Exception as e:
            # Rediséšœå®³æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if fallback_func:
                return await fallback_func()
        return None

    async def set_with_compression(self, key: str, value: Any, ttl: int):
        """åœ§ç¸®ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š"""
        try:
            serialized = pickle.dumps(value)
            # å¤§ããªãƒ‡ãƒ¼ã‚¿ã¯åœ§ç¸®
            if len(serialized) > 1024:
                import gzip
                serialized = gzip.compress(serialized)
                key += ":gz"

            await self.client.setex(key, ttl, serialized)
        except Exception as e:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¤±æ•—ã¯éè‡´å‘½çš„
            pass

    async def get_pool_stats(self) -> dict:
        """æ¥ç¶šãƒ—ãƒ¼ãƒ«çµ±è¨ˆ"""
        info = await self.client.info()
        return {
            "connected_clients": info.get("connected_clients", 0),
            "used_memory_human": info.get("used_memory_human", "0B"),
            "keyspace_hits": info.get("keyspace_hits", 0),
            "keyspace_misses": info.get("keyspace_misses", 0),
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
redis_client = OptimizedRedisClient()
```

### 3. LLM ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…

**å®Ÿè£…ä¾‹**:

```python
# src/application/llm_integration/services/cached_llm_service.py
import hashlib
from typing import Optional, Dict, Any
from ...infrastructure.shared.cache.redis_client import redis_client

class CachedLLMService:
    def __init__(self):
        self.cache_ttls = {
            "prompt_response": 86400,    # 24æ™‚é–“
            "evaluation_result": 43200,  # 12æ™‚é–“
            "system_prompt": 604800,     # 1é€±é–“
        }

    def _generate_cache_key(self, prompt: str, model: str, params: Dict) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆï¼ˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰"""
        content = f"{prompt}:{model}:{sorted(params.items())}"
        return f"llm:{hashlib.md5(content.encode()).hexdigest()}"

    async def get_cached_response(self, prompt: str, model: str, params: Dict) -> Optional[str]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—"""
        cache_key = self._generate_cache_key(prompt, model, params)
        return await redis_client.get_with_fallback(cache_key)

    async def cache_response(self, prompt: str, model: str, params: Dict, response: str):
        """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        cache_key = self._generate_cache_key(prompt, model, params)
        ttl = self.cache_ttls.get("prompt_response", 3600)
        await redis_client.set_with_compression(cache_key, response, ttl)

    async def get_cache_stats(self) -> Dict[str, int]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆå–å¾—"""
        stats = await redis_client.get_pool_stats()
        hit_rate = 0
        if stats["keyspace_hits"] + stats["keyspace_misses"] > 0:
            hit_rate = stats["keyspace_hits"] / (stats["keyspace_hits"] + stats["keyspace_misses"])

        return {
            "hit_rate_percent": round(hit_rate * 100, 2),
            "total_hits": stats["keyspace_hits"],
            "total_misses": stats["keyspace_misses"],
        }
```

## ğŸš€ High Priority å®Ÿè£…

### 4. WebSocket æ¥ç¶šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

**å®Ÿè£…ä¾‹**:

```python
# src/presentation/websocket/connection_manager.py
import asyncio
import json
import logging
from typing import Dict, Set, Optional
from fastapi import WebSocket, WebSocketDisconnect
from collections import defaultdict

logger = logging.getLogger(__name__)

class OptimizedConnectionManager:
    def __init__(self, max_connections: int = 15000):
        self.active_connections: Dict[str, WebSocket] = {}
        self.user_connections: Dict[str, Set[str]] = defaultdict(set)
        self.max_connections = max_connections
        self.connection_count = 0
        self.broadcast_queue = asyncio.Queue(maxsize=1000)

        # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒ¯ãƒ¼ã‚«ãƒ¼é–‹å§‹
        asyncio.create_task(self._broadcast_worker())

    async def connect(self, websocket: WebSocket, client_id: str, user_id: Optional[str] = None):
        """WebSocketæ¥ç¶šç¢ºç«‹"""
        if self.connection_count >= self.max_connections:
            await websocket.close(code=1013, reason="Server overloaded")
            return False

        await websocket.accept()
        self.active_connections[client_id] = websocket
        self.connection_count += 1

        if user_id:
            self.user_connections[user_id].add(client_id)

        logger.info(f"Client {client_id} connected. Total: {self.connection_count}")
        return True

    async def disconnect(self, client_id: str, user_id: Optional[str] = None):
        """WebSocketæ¥ç¶šåˆ‡æ–­"""
        if client_id in self.active_connections:
            del self.active_connections[client_id]
            self.connection_count -= 1

        if user_id and client_id in self.user_connections[user_id]:
            self.user_connections[user_id].remove(client_id)
            if not self.user_connections[user_id]:
                del self.user_connections[user_id]

    async def send_to_user(self, user_id: str, message: dict):
        """ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡"""
        if user_id not in self.user_connections:
            return

        message_str = json.dumps(message)
        tasks = []

        for client_id in self.user_connections[user_id]:
            if client_id in self.active_connections:
                websocket = self.active_connections[client_id]
                tasks.append(self._safe_send(websocket, message_str, client_id))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

    async def broadcast(self, message: dict):
        """å…¨æ¥ç¶šã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆï¼ˆéåŒæœŸã‚­ãƒ¥ãƒ¼ä½¿ç”¨ï¼‰"""
        await self.broadcast_queue.put(message)

    async def _broadcast_worker(self):
        """ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        while True:
            try:
                message = await self.broadcast_queue.get()
                message_str = json.dumps(message)

                # ãƒãƒƒãƒå‡¦ç†ã§åŠ¹ç‡çš„ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
                batch_size = 100
                connection_items = list(self.active_connections.items())

                for i in range(0, len(connection_items), batch_size):
                    batch = connection_items[i:i + batch_size]
                    tasks = [
                        self._safe_send(websocket, message_str, client_id)
                        for client_id, websocket in batch
                    ]
                    await asyncio.gather(*tasks, return_exceptions=True)

                    # CPUè² è·è»½æ¸›ã®ãŸã‚ã®çŸ­ã„ä¼‘æ†©
                    if len(connection_items) > 1000:
                        await asyncio.sleep(0.001)

            except Exception as e:
                logger.error(f"Broadcast worker error: {e}")

    async def _safe_send(self, websocket: WebSocket, message: str, client_id: str):
        """å®‰å…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        try:
            await websocket.send_text(message)
        except WebSocketDisconnect:
            await self.disconnect(client_id)
        except Exception as e:
            logger.warning(f"Failed to send to {client_id}: {e}")
            await self.disconnect(client_id)

    def get_stats(self) -> dict:
        """æ¥ç¶šçµ±è¨ˆå–å¾—"""
        return {
            "total_connections": self.connection_count,
            "unique_users": len(self.user_connections),
            "max_connections": self.max_connections,
            "utilization_percent": round((self.connection_count / self.max_connections) * 100, 2),
            "queue_size": self.broadcast_queue.qsize(),
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
connection_manager = OptimizedConnectionManager()
```

### 5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†

**å®Ÿè£…ä¾‹**:

```python
# src/application/shared/services/memory_optimizer.py
import gc
import asyncio
import psutil
import logging
from typing import AsyncGenerator, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class MemoryStats:
    used_mb: float
    available_mb: float
    percent: float
    process_mb: float

class MemoryOptimizer:
    def __init__(self, memory_threshold: float = 85.0):
        self.memory_threshold = memory_threshold
        self.cleanup_interval = 300  # 5åˆ†é–“éš”
        asyncio.create_task(self._periodic_cleanup())

    async def stream_large_data(self, data_source) -> AsyncGenerator[Any, None]:
        """å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†"""
        chunk_size = 1000

        async for chunk in self._chunk_data(data_source, chunk_size):
            yield chunk

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
            if await self._should_cleanup():
                await self._force_cleanup()

    async def _chunk_data(self, data_source, chunk_size: int):
        """ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã‚µã‚¤ã‚ºã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        chunk = []
        async for item in data_source:
            chunk.append(item)
            if len(chunk) >= chunk_size:
                yield chunk
                chunk = []

        if chunk:
            yield chunk

    async def _should_cleanup(self) -> bool:
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯"""
        memory_stats = await self.get_memory_stats()
        return memory_stats.percent > self.memory_threshold

    async def _force_cleanup(self):
        """å¼·åˆ¶ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        logger.info("Performing memory cleanup")

        # Python GCå®Ÿè¡Œ
        collected = gc.collect()

        # ãƒ—ãƒ­ã‚»ã‚¹å›ºæœ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await self._cleanup_application_cache()

        logger.info(f"Memory cleanup completed. Collected {collected} objects")

    async def _cleanup_application_cache(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å›ºæœ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆã‚¢ã®å¤ã„ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
        # ä¸€æ™‚çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        pass

    async def _periodic_cleanup(self):
        """å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        while True:
            await asyncio.sleep(self.cleanup_interval)
            if await self._should_cleanup():
                await self._force_cleanup()

    async def get_memory_stats(self) -> MemoryStats:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³å–å¾—"""
        memory = psutil.virtual_memory()
        process = psutil.Process()

        return MemoryStats(
            used_mb=memory.used / 1024 / 1024,
            available_mb=memory.available / 1024 / 1024,
            percent=memory.percent,
            process_mb=process.memory_info().rss / 1024 / 1024
        )

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
memory_optimizer = MemoryOptimizer()
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–å¼·åŒ–

### 6. è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

**å®Ÿè£…ä¾‹**:

```python
# src/monitoring/performance_monitor.py
import time
import asyncio
from typing import Dict, List
from dataclasses import dataclass, asdict
from collections import deque, defaultdict

@dataclass
class RequestMetrics:
    endpoint: str
    method: str
    status_code: int
    duration_ms: float
    timestamp: float
    user_id: str = None

class PerformanceMonitor:
    def __init__(self, max_metrics: int = 10000):
        self.request_metrics: deque = deque(maxlen=max_metrics)
        self.endpoint_stats: Dict[str, List[float]] = defaultdict(list)
        self.max_stats_per_endpoint = 1000

    def record_request(self, metrics: RequestMetrics):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
        self.request_metrics.append(metrics)

        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆåˆ¥çµ±è¨ˆæ›´æ–°
        endpoint_key = f"{metrics.method} {metrics.endpoint}"
        self.endpoint_stats[endpoint_key].append(metrics.duration_ms)

        # å¤ã„ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
        if len(self.endpoint_stats[endpoint_key]) > self.max_stats_per_endpoint:
            self.endpoint_stats[endpoint_key] = self.endpoint_stats[endpoint_key][-self.max_stats_per_endpoint:]

    def get_performance_summary(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚µãƒãƒªãƒ¼"""
        summary = {}

        for endpoint, durations in self.endpoint_stats.items():
            if durations:
                sorted_durations = sorted(durations)
                count = len(sorted_durations)

                summary[endpoint] = {
                    "count": count,
                    "avg_ms": sum(sorted_durations) / count,
                    "p50_ms": sorted_durations[int(count * 0.5)],
                    "p90_ms": sorted_durations[int(count * 0.9)],
                    "p95_ms": sorted_durations[int(count * 0.95)],
                    "p99_ms": sorted_durations[int(count * 0.99)],
                    "max_ms": max(sorted_durations),
                    "min_ms": min(sorted_durations),
                }

        return summary

    def get_slow_requests(self, threshold_ms: float = 200) -> List[Dict]:
        """é–¾å€¤ã‚’è¶…ãˆã‚‹é…ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆå–å¾—"""
        slow_requests = [
            asdict(metric) for metric in self.request_metrics
            if metric.duration_ms > threshold_ms
        ]
        return sorted(slow_requests, key=lambda x: x["duration_ms"], reverse=True)[:50]

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
performance_monitor = PerformanceMonitor()
```

## ğŸ§ª ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè£…

### 7. è² è·ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

**å®Ÿè£…ä¾‹**:

```python
# tests/performance/test_load.py
import asyncio
import pytest
import aiohttp
import time
from typing import List, Dict

class LoadTestRunner:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[Dict] = []

    async def run_concurrent_requests(self, endpoint: str, concurrent_users: int, requests_per_user: int):
        """ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        tasks = []

        for user_id in range(concurrent_users):
            task = asyncio.create_task(
                self._user_session(endpoint, requests_per_user, user_id)
            )
            tasks.append(task)

        await asyncio.gather(*tasks)

    async def _user_session(self, endpoint: str, request_count: int, user_id: int):
        """å€‹åˆ¥ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³"""
        async with aiohttp.ClientSession() as session:
            for i in range(request_count):
                start_time = time.time()

                try:
                    async with session.get(f"{self.base_url}{endpoint}") as response:
                        await response.text()
                        duration = time.time() - start_time

                        self.results.append({
                            "user_id": user_id,
                            "request_id": i,
                            "status_code": response.status,
                            "duration_ms": duration * 1000,
                            "timestamp": start_time
                        })

                except Exception as e:
                    duration = time.time() - start_time
                    self.results.append({
                        "user_id": user_id,
                        "request_id": i,
                        "status_code": 0,
                        "duration_ms": duration * 1000,
                        "error": str(e),
                        "timestamp": start_time
                    })

    def analyze_results(self) -> Dict:
        """è² è·ãƒ†ã‚¹ãƒˆçµæœåˆ†æ"""
        if not self.results:
            return {}

        durations = [r["duration_ms"] for r in self.results if r.get("status_code", 0) == 200]
        errors = [r for r in self.results if r.get("status_code", 0) != 200]

        if durations:
            durations.sort()
            count = len(durations)

            return {
                "total_requests": len(self.results),
                "successful_requests": len(durations),
                "error_requests": len(errors),
                "success_rate": len(durations) / len(self.results) * 100,
                "avg_response_time_ms": sum(durations) / count,
                "p50_ms": durations[int(count * 0.5)],
                "p90_ms": durations[int(count * 0.9)],
                "p95_ms": durations[int(count * 0.95)],
                "p99_ms": durations[int(count * 0.99)],
                "max_ms": max(durations),
                "min_ms": min(durations),
            }

        return {"error": "No successful requests"}

@pytest.mark.asyncio
async def test_api_load_performance():
    """APIè² è·ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    runner = LoadTestRunner()

    # 100ä¸¦è¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã€å„10ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    await runner.run_concurrent_requests("/", 100, 10)

    results = runner.analyze_results()

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶æ¤œè¨¼
    assert results["success_rate"] > 95, f"Success rate too low: {results['success_rate']}%"
    assert results["p95_ms"] < 200, f"P95 response time too high: {results['p95_ms']}ms"
    assert results["avg_response_time_ms"] < 100, f"Average response time too high: {results['avg_response_time_ms']}ms"
```

## ğŸ“ˆ å®Ÿè£…å„ªå…ˆé †ä½ã¨åŠ¹æœäºˆæ¸¬

| å„ªå…ˆåº¦ | å®Ÿè£…é …ç›® | äºˆæƒ³åŠ¹æœ | å®Ÿè£…æ™‚é–“ |
|-------|---------|---------|----------|
| 1 | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ« | P95: 300ms â†’ 150ms | 2æ—¥ |
| 1 | Redisæ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ– | ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: 30% â†’ 80% | 1æ—¥ |
| 2 | LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | LLMå¾…æ©Ÿæ™‚é–“: 90%å‰Šæ¸› | 3æ—¥ |
| 2 | WebSocketæ¥ç¶šç®¡ç† | åŒæ™‚æ¥ç¶š: 500 â†’ 10,000+ | 5æ—¥ |
| 3 | ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 40%å‰Šæ¸› | 3æ—¥ |
| 3 | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦– | å•é¡Œæ¤œå‡ºæ™‚é–“: 90%çŸ­ç¸® | 2æ—¥ |

**ç·åˆäºˆæ¸¬åŠ¹æœ**: P95ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ 60%æ”¹å–„ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡40%å‰Šæ¸›ã€åŒæ™‚æ¥ç¶šæ•°20å€å‘ä¸Š

---

**å®Ÿè£…é–‹å§‹äºˆå®š**: 2025-01-20
**å®Œäº†ç›®æ¨™**: 2025-02-15
**è²¬ä»»è€…**: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºãƒãƒ¼ãƒ 