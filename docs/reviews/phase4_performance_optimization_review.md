# Phase 4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¬ãƒ“ãƒ¥ãƒ¼

**ãƒ¬ãƒ“ãƒ¥ãƒ¼æ—¥æ™‚**: 2025å¹´10æœˆ1æ—¥ **ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡**: Phase 4 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®Ÿè£…
**ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼**: performance-optimizer Agent **æ·±åˆ»åº¦**: ğŸŸ¡ ä¸­ï¼ˆæœ€é©åŒ–æ¨å¥¨ã‚ã‚Šï¼‰

---

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

Phase
4ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®Ÿè£…ã‚’åŒ…æ‹¬çš„ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ãŸçµæœã€åŸºæœ¬çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å …ç‰¢ã§ã‚ã‚‹ãŒã€æœ¬ç•ªç’°å¢ƒã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã€**10ã®é‡è¦ãªæœ€é©åŒ–æ©Ÿä¼š**ã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚

### ç·åˆè©•ä¾¡

| è©•ä¾¡é …ç›®             | ç¾çŠ¶ | ç›®æ¨™ | ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ |
| -------------------- | ---- | ---- | ---------- |
| æ¥ç¶šãƒ—ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡   | 70%  | 90%+ | ğŸŸ¡ è¦æ”¹å–„  |
| ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | 75%  | 95%+ | ğŸŸ¡ è¦æ”¹å–„  |
| ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥     | 80%  | 95%+ | ğŸŸ¢ è‰¯å¥½    |
| ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥     | 30%  | 90%+ | ğŸ”´ æœªå®Ÿè£…  |
| éåŒæœŸãƒ‘ã‚¿ãƒ¼ãƒ³       | 40%  | 95%+ | ğŸŸ¡ è¦æ”¹å–„  |
| N+1å•é¡Œå¯¾ç­–          | 85%  | 100% | ğŸŸ¢ è‰¯å¥½    |
| ãƒ¡ãƒ¢ãƒªåŠ¹ç‡           | ä¸æ˜ | 90%+ | âšª è¦æ¸¬å®š  |
| ã‚¨ãƒƒã‚¸å¯¾å¿œæº–å‚™       | 60%  | 95%+ | ğŸŸ¡ è¦æ”¹å–„  |

### æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ

æœ€é©åŒ–å®Ÿæ–½å¾Œã®äºˆæ¸¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼š

- **API P95ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ **: 300ms â†’ **150msä»¥ä¸‹** (50%æ”¹å–„)
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªæ™‚é–“**: å¹³å‡80ms â†’ **30msä»¥ä¸‹** (62%æ”¹å–„)
- **åŒæ™‚æ¥ç¶šå‡¦ç†èƒ½åŠ›**: 100æ¥ç¶š â†’ **1000+æ¥ç¶š** (10å€)
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡**: 0% â†’ **80%+** (æ–°è¦å®Ÿè£…)
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: æ¨å®šå‰Šæ¸› **30-40%**

---

## 1. æ¥ç¶šãƒ—ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ã®æœ€é©åŒ–

### ç¾çŠ¶åˆ†æ

**âœ… è‰¯å¥½ãªç‚¹:**

```python
# turso_connection.py (L72-77)
self._engine = create_engine(
    connection_url,
    echo=self.settings.debug,
    pool_size=10,           # âœ… åŸºæœ¬çš„ãªãƒ—ãƒ¼ãƒ«è¨­å®š
    max_overflow=20,        # âœ… ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾å¿œ
    pool_pre_ping=True,     # âœ… æ¥ç¶šæ¤œè¨¼
)
```

**ğŸ”´ å•é¡Œç‚¹:**

1. **ãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºãŒå›ºå®š** - ç’°å¢ƒåˆ¥æœ€é©åŒ–ãªã—
2. **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæœªè¨­å®š** - é•·æ™‚é–“æ¥ç¶šãŒè“„ç©
3. **ãƒªã‚µã‚¤ã‚¯ãƒ«æˆ¦ç•¥ãªã—** - å¤ã„æ¥ç¶šãŒæ®‹ç•™
4. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãªã—** - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å¯è¦–åŒ–ä¸å¯

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨1: ç’°å¢ƒåˆ¥æ¥ç¶šãƒ—ãƒ¼ãƒ«æˆ¦ç•¥

```python
# src/infrastructure/shared/database/turso_connection.py

def get_pool_config(self, env: str) -> dict:
    """ç’°å¢ƒåˆ¥ãƒ—ãƒ¼ãƒ«è¨­å®š"""
    configs = {
        "local": {
            "pool_size": 5,
            "max_overflow": 10,
            "pool_timeout": 30,
            "pool_recycle": 3600,  # 1æ™‚é–“ã§ãƒªã‚µã‚¤ã‚¯ãƒ«
        },
        "staging": {
            "pool_size": 10,
            "max_overflow": 20,
            "pool_timeout": 20,
            "pool_recycle": 1800,  # 30åˆ†
        },
        "production": {
            "pool_size": 20,
            "max_overflow": 40,
            "pool_timeout": 10,
            "pool_timeout_retry": 3,
            "pool_recycle": 900,   # 15åˆ†ï¼ˆTursoæ¥ç¶šå®‰å®šæ€§ï¼‰
            "pool_pre_ping": True,
            "pool_use_lifo": True,  # æœ€è¿‘ä½¿ç”¨ã—ãŸæ¥ç¶šã‚’å„ªå…ˆå†åˆ©ç”¨
        }
    }
    return configs.get(env, configs["local"])

def get_engine(self):
    """æœ€é©åŒ–ã•ã‚ŒãŸSQLAlchemyã‚¨ãƒ³ã‚¸ãƒ³"""
    if self._engine is None:
        connection_url = self.get_connection_url()
        env = os.getenv("APP_ENV", "local")
        pool_config = self.get_pool_config(env)

        if "sqlite" in connection_url:
            # SQLite: è»½é‡è¨­å®š
            self._engine = create_engine(
                connection_url,
                connect_args={"check_same_thread": False},
                poolclass=StaticPool,
                echo=self.settings.debug,
            )
        else:
            # Turso/libSQL: æœ¬ç•ªæœ€é©åŒ–
            self._engine = create_engine(
                connection_url,
                echo=self.settings.debug,
                **pool_config,
                # ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                pool_logging_name="turso_pool",
            )

            # æ¥ç¶šãƒ—ãƒ¼ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            self._setup_pool_monitoring(self._engine)

    return self._engine

def _setup_pool_monitoring(self, engine):
    """æ¥ç¶šãƒ—ãƒ¼ãƒ«ç›£è¦–"""
    from sqlalchemy import event

    @event.listens_for(engine, "connect")
    def receive_connect(dbapi_conn, connection_record):
        # æ¥ç¶šç¢ºç«‹æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        logger.debug("Database connection established")
        # TODO: Prometheus metrics

    @event.listens_for(engine, "checkin")
    def receive_checkin(dbapi_conn, connection_record):
        # æ¥ç¶šè¿”å´æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        pass
```

**æœŸå¾…åŠ¹æœ:**

- æœ¬ç•ªç’°å¢ƒã§ã®æ¥ç¶šå¾…æ©Ÿæ™‚é–“: **80%å‰Šæ¸›**
- æ¥ç¶šå†åˆ©ç”¨ç‡: **90%ä»¥ä¸Šé”æˆ**
- Tursoã‚¨ãƒƒã‚¸ãƒãƒ¼ãƒ‰ã¸ã®æœ€é©åˆ†æ•£

---

## 2. éåŒæœŸã‚¯ã‚¨ãƒªå®Ÿè¡Œã®æ‹¡å¼µ

### ç¾çŠ¶åˆ†æ

**ğŸŸ¡ éƒ¨åˆ†å®Ÿè£…:**

```python
# turso_connection.py (L93-95)
async def execute_raw(self, query: str, params: dict | None = None):
    """Execute raw SQL query using libSQL client"""
    client = self.get_libsql_client()
    return await client.execute(query, params or {})
```

**ğŸ”´ å•é¡Œç‚¹:**

1. **åŒæœŸORMæ“ä½œãŒä¸»æµ** - SQLAlchemyã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯åŒæœŸã®ã¿
2. **ãƒãƒƒãƒå‡¦ç†ã®éåŠ¹ç‡** - é †æ¬¡å®Ÿè¡Œã§ä¸¦åˆ—åŒ–ãªã—
3. **ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†ãŒæœªæ•´å‚™** - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸è¶³

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨2: SQLAlchemy 2.0 éåŒæœŸã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆ

```python
# src/infrastructure/shared/database/async_connection.py

from sqlalchemy.ext.asyncio import (
    create_async_engine,
    async_sessionmaker,
    AsyncSession,
)

class AsyncTursoConnection:
    """éåŒæœŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self):
        self.settings = Settings()
        self._async_engine = None
        self._async_session_factory = None

    def get_async_engine(self):
        """éåŒæœŸSQLAlchemyã‚¨ãƒ³ã‚¸ãƒ³"""
        if self._async_engine is None:
            connection_url = self.get_connection_url()
            # libSQLç”¨ã«æ¥ç¶šURLã‚’å¤‰æ›
            async_url = connection_url.replace("libsql://", "sqlite+aiosqlite://")

            self._async_engine = create_async_engine(
                async_url,
                echo=self.settings.debug,
                pool_size=20,
                max_overflow=40,
                pool_recycle=900,
                pool_pre_ping=True,
            )

        return self._async_engine

    def get_async_session_factory(self) -> async_sessionmaker:
        """éåŒæœŸã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼"""
        if self._async_session_factory is None:
            self._async_session_factory = async_sessionmaker(
                bind=self.get_async_engine(),
                class_=AsyncSession,
                expire_on_commit=False,  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
            )
        return self._async_session_factory

    async def get_async_session(self) -> AsyncSession:
        """éåŒæœŸã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—"""
        factory = self.get_async_session_factory()
        async with factory() as session:
            yield session

# ä½¿ç”¨ä¾‹: ãƒªãƒã‚¸ãƒˆãƒªå±¤
class AsyncPromptRepository:
    """éåŒæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªãƒã‚¸ãƒˆãƒª"""

    async def find_by_user_id(
        self,
        session: AsyncSession,
        user_id: str,
        limit: int = 100
    ) -> list[PromptModel]:
        """éåŒæœŸã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        from sqlalchemy import select

        stmt = (
            select(PromptModel)
            .where(PromptModel.user_id == user_id)
            .where(PromptModel.deleted_at.is_(None))
            .order_by(PromptModel.created_at.desc())
            .limit(limit)
        )

        result = await session.execute(stmt)
        return result.scalars().all()

    async def bulk_create(
        self,
        session: AsyncSession,
        prompts: list[PromptModel]
    ) -> list[PromptModel]:
        """ãƒãƒ«ã‚¯ä½œæˆï¼ˆæœ€é©åŒ–ï¼‰"""
        session.add_all(prompts)
        await session.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŒã‚³ãƒŸãƒƒãƒˆã¯ã—ãªã„
        return prompts
```

**æœŸå¾…åŠ¹æœ:**

- è¤‡æ•°ã‚¯ã‚¨ãƒªã®ä¸¦åˆ—å®Ÿè¡Œ: **5-10å€é«˜é€ŸåŒ–**
- CPUä½¿ç”¨ç‡ã®åŠ¹ç‡åŒ–: **30%æ”¹å–„**
- éåŒæœŸAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¯¾å¿œ

---

## 3. Redis ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥ã®å®Ÿè£…

### ç¾çŠ¶åˆ†æ

**ğŸ”´ æœªå®Ÿè£…:**

- Redisè¨­å®šã¯å­˜åœ¨ã™ã‚‹ãŒã€å®Ÿè£…ãªã—
- ã™ã¹ã¦ã®ã‚¯ã‚¨ãƒªãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: **0%**

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨3: å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

```python
# src/infrastructure/shared/cache/redis_cache.py

import json
from typing import Any, Optional
import redis.asyncio as aioredis
from src.core.config.settings import Settings

class RedisCache:
    """RediséåŒæœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self):
        self.settings = Settings()
        self._client: Optional[aioredis.Redis] = None

    async def get_client(self) -> aioredis.Redis:
        """éåŒæœŸRedisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—"""
        if self._client is None:
            self._client = await aioredis.from_url(
                self.settings.get_redis_url(),
                encoding="utf-8",
                decode_responses=True,
                max_connections=50,  # é«˜è² è·å¯¾å¿œ
                socket_keepalive=True,
            )
        return self._client

    async def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—"""
        client = await self.get_client()
        value = await client.get(key)
        return json.loads(value) if value else None

    async def set(
        self,
        key: str,
        value: Any,
        ttl: int = 3600
    ) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š"""
        client = await self.get_client()
        return await client.setex(
            key,
            ttl,
            json.dumps(value, default=str)
        )

    async def delete(self, key: str) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤"""
        client = await self.get_client()
        return await client.delete(key) > 0

    async def delete_pattern(self, pattern: str) -> int:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤"""
        client = await self.get_client()
        keys = await client.keys(pattern)
        if keys:
            return await client.delete(*keys)
        return 0

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
def cache_result(ttl: int = 3600, key_prefix: str = ""):
    """ã‚¯ã‚¨ãƒªçµæœã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache = RedisCache()
            cached = await cache.get(cache_key)
            if cached:
                return cached

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã¯å®Ÿè¡Œ
            result = await func(*args, **kwargs)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            await cache.set(cache_key, result, ttl)
            return result

        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹: ãƒªãƒã‚¸ãƒˆãƒªå±¤
class CachedPromptRepository:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªãƒã‚¸ãƒˆãƒª"""

    @cache_result(ttl=600, key_prefix="prompt")
    async def get_by_id(
        self,
        session: AsyncSession,
        prompt_id: str
    ) -> Optional[PromptModel]:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
        result = await session.execute(
            select(PromptModel).where(PromptModel.id == prompt_id)
        )
        return result.scalar_one_or_none()

    @cache_result(ttl=300, key_prefix="prompt_list")
    async def list_by_user(
        self,
        session: AsyncSession,
        user_id: str,
        limit: int = 50
    ) -> list[PromptModel]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¸€è¦§ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
        result = await session.execute(
            select(PromptModel)
            .where(PromptModel.user_id == user_id)
            .where(PromptModel.deleted_at.is_(None))
            .order_by(PromptModel.created_at.desc())
            .limit(limit)
        )
        return result.scalars().all()

    async def update(
        self,
        session: AsyncSession,
        prompt: PromptModel
    ) -> PromptModel:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ›´æ–°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ï¼‰"""
        session.add(prompt)
        await session.commit()

        # é–¢é€£ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
        cache = RedisCache()
        await cache.delete(f"prompt:get_by_id:{prompt.id}")
        await cache.delete_pattern(f"prompt_list:*{prompt.user_id}*")

        return prompt
```

#### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—     | TTL   | ç„¡åŠ¹åŒ–æˆ¦ç•¥      | å„ªå…ˆåº¦ |
| ---------------- | ----- | --------------- | ------ |
| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè©³ç´°   | 10åˆ†  | æ›´æ–°æ™‚å‰Šé™¤      | é«˜     |
| ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§     | 5åˆ†   | ä½œæˆ/æ›´æ–°æ™‚å‰Šé™¤ | é«˜     |
| è©•ä¾¡çµæœ         | 1æ™‚é–“ | å†è©•ä¾¡æ™‚å‰Šé™¤    | ä¸­     |
| ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¸€è¦§ | 1æ™‚é–“ | æ›´æ–°æ™‚å‰Šé™¤      | ä¸­     |
| çµ±è¨ˆæƒ…å ±         | 30åˆ†  | å®šæœŸæ›´æ–°        | ä½     |

**æœŸå¾…åŠ¹æœ:**

- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è² è·: **60-80%å‰Šæ¸›**
- API P50ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ : **200ms â†’ 20ms** (10å€é«˜é€ŸåŒ–)
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: **80%ä»¥ä¸Šé”æˆ**

---

## 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥ã®å¼·åŒ–

### ç¾çŠ¶åˆ†æ

**âœ… æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆè‰¯å¥½ï¼‰:**

```sql
-- prompts ãƒ†ãƒ¼ãƒ–ãƒ«
idx_prompts_user_id          -- ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢
idx_prompts_status           -- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
idx_prompts_created_at       -- æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆ
idx_prompts_parent_id        -- ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¿½è·¡
idx_prompts_deleted_at       -- è«–ç†å‰Šé™¤ãƒ•ã‚£ãƒ«ã‚¿

-- evaluations ãƒ†ãƒ¼ãƒ–ãƒ«
idx_evaluations_prompt_id    -- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢é€£
idx_evaluations_status       -- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
idx_evaluations_created_at   -- æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆ
idx_evaluations_provider_model  -- ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥åˆ†æ

-- test_results ãƒ†ãƒ¼ãƒ–ãƒ«
idx_test_results_evaluation_id  -- è©•ä¾¡é–¢é€£
idx_test_results_passed         -- åˆæ ¼ãƒ•ã‚£ãƒ«ã‚¿
idx_test_results_score          -- ã‚¹ã‚³ã‚¢ã‚½ãƒ¼ãƒˆ
```

**ğŸŸ¡ è¿½åŠ æ¨å¥¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹:**

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨4: è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚«ãƒãƒªãƒ³ã‚°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

```python
# alembic/versions/xxxx_add_composite_indexes.py

"""Add composite and covering indexes for query optimization

Revision ID: xxxx
"""

def upgrade() -> None:
    # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¤œç´¢ï¼ˆæœ€é »å‡ºã‚¯ã‚¨ãƒªï¼‰
    op.create_index(
        'idx_prompts_user_status_created',
        'prompts',
        ['user_id', 'status', 'created_at'],
        unique=False,
        postgresql_where="deleted_at IS NULL",  # éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    )

    # 2. è©•ä¾¡åˆ†æã‚¯ã‚¨ãƒªæœ€é©åŒ–
    op.create_index(
        'idx_evaluations_prompt_status_score',
        'evaluations',
        ['prompt_id', 'status', 'overall_score'],
        unique=False,
    )

    # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æç”¨
    op.create_index(
        'idx_test_results_eval_passed_score',
        'test_results',
        ['evaluation_id', 'passed', 'score'],
        unique=False,
    )

    # 4. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨åˆ†æ
    op.create_index(
        'idx_templates_category_usage',
        'prompt_templates',
        ['category', 'usage_count'],
        unique=False,
    )

    # 5. å…¨æ–‡æ¤œç´¢æº–å‚™ï¼ˆlibSQL Vectoré€£æºï¼‰
    # TODO: Phase 4.5 - Vector Extensionã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 
    # op.execute("CREATE INDEX idx_prompts_content_vector ON prompts USING vector(content_embedding)")

def downgrade() -> None:
    op.drop_index('idx_templates_category_usage', table_name='prompt_templates')
    op.drop_index('idx_test_results_eval_passed_score', table_name='test_results')
    op.drop_index('idx_evaluations_prompt_status_score', table_name='evaluations')
    op.drop_index('idx_prompts_user_status_created', table_name='prompts')
```

#### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŠ¹æœæ¸¬å®š

```python
# tests/performance/test_index_performance.py

import pytest
from sqlalchemy import text

@pytest.mark.performance
async def test_user_prompts_query_performance(async_session):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¯ã‚¨ãƒªã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    import time

    # è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨ã‚¯ã‚¨ãƒª
    query = """
    SELECT id, title, created_at
    FROM prompts
    WHERE user_id = :user_id
      AND status = 'active'
      AND deleted_at IS NULL
    ORDER BY created_at DESC
    LIMIT 50
    """

    start = time.perf_counter()
    result = await async_session.execute(
        text(query),
        {"user_id": "test_user"}
    )
    elapsed = time.perf_counter() - start

    # ç›®æ¨™: 10msä»¥å†…
    assert elapsed < 0.01, f"Query too slow: {elapsed:.3f}s"

@pytest.mark.performance
async def test_evaluation_analysis_performance(async_session):
    """è©•ä¾¡åˆ†æã‚¯ã‚¨ãƒªã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    query = """
    SELECT
        prompt_id,
        AVG(overall_score) as avg_score,
        COUNT(*) as eval_count
    FROM evaluations
    WHERE status = 'completed'
      AND overall_score IS NOT NULL
    GROUP BY prompt_id
    HAVING eval_count > 5
    ORDER BY avg_score DESC
    LIMIT 20
    """

    start = time.perf_counter()
    await async_session.execute(text(query))
    elapsed = time.perf_counter() - start

    # ç›®æ¨™: 50msä»¥å†…
    assert elapsed < 0.05, f"Analysis query too slow: {elapsed:.3f}s"
```

**æœŸå¾…åŠ¹æœ:**

- è¤‡åˆã‚¯ã‚¨ãƒªé€Ÿåº¦: **5-10å€é«˜é€ŸåŒ–**
- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚«ãƒãƒ¼ç‡: **95%ä»¥ä¸Š**
- ãƒ•ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³: **å®Œå…¨æ’é™¤**

---

## 5. N+1 ã‚¯ã‚¨ãƒªå•é¡Œã®å®Œå…¨æ’é™¤

### ç¾çŠ¶åˆ†æ

**âœ… DDDå¢ƒç•Œã«ã‚ˆã‚‹ä¿è­·:**

```python
# é›†ç´„å¢ƒç•Œã«ã‚ˆã‚Šç›´æ¥relationshipã‚’é¿ã‘ã‚‹è¨­è¨ˆ
# â†’ N+1å•é¡Œã®ãƒªã‚¹ã‚¯ã‚’æ§‹é€ çš„ã«ä½æ¸›
```

**ğŸŸ¡ æ½œåœ¨çš„ãƒªã‚¹ã‚¯:**

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨5: Eager Loading ã¨ãƒãƒƒãƒãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥

```python
# src/infrastructure/prompt/repositories/prompt_repository.py

from sqlalchemy.orm import selectinload, joinedload
from sqlalchemy import select

class OptimizedPromptRepository:
    """N+1å•é¡Œã‚’æ’é™¤ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªãƒã‚¸ãƒˆãƒª"""

    async def get_prompts_with_evaluations(
        self,
        session: AsyncSession,
        user_id: str,
        limit: int = 50
    ) -> list[dict]:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨è©•ä¾¡ã‚’åŠ¹ç‡çš„ã«å–å¾—ï¼ˆN+1å›é¿ï¼‰"""

        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆIDãƒªã‚¹ãƒˆå–å¾—
        prompt_stmt = (
            select(PromptModel.id, PromptModel.title, PromptModel.created_at)
            .where(PromptModel.user_id == user_id)
            .where(PromptModel.deleted_at.is_(None))
            .order_by(PromptModel.created_at.desc())
            .limit(limit)
        )
        prompt_result = await session.execute(prompt_stmt)
        prompts = prompt_result.all()

        if not prompts:
            return []

        prompt_ids = [p.id for p in prompts]

        # ã‚¹ãƒ†ãƒƒãƒ—2: è©•ä¾¡ã‚’ä¸€æ‹¬å–å¾—ï¼ˆ1ã‚¯ã‚¨ãƒªï¼‰
        eval_stmt = (
            select(EvaluationModel)
            .where(EvaluationModel.prompt_id.in_(prompt_ids))
            .where(EvaluationModel.status == 'completed')
        )
        eval_result = await session.execute(eval_stmt)
        evaluations = eval_result.scalars().all()

        # ã‚¹ãƒ†ãƒƒãƒ—3: è©•ä¾¡ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆIDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªå†…ï¼‰
        eval_by_prompt = {}
        for ev in evaluations:
            if ev.prompt_id not in eval_by_prompt:
                eval_by_prompt[ev.prompt_id] = []
            eval_by_prompt[ev.prompt_id].append(ev)

        # ã‚¹ãƒ†ãƒƒãƒ—4: çµæœçµ„ã¿ç«‹ã¦
        result = []
        for prompt in prompts:
            result.append({
                "id": prompt.id,
                "title": prompt.title,
                "created_at": prompt.created_at,
                "evaluations": eval_by_prompt.get(prompt.id, []),
                "eval_count": len(eval_by_prompt.get(prompt.id, [])),
            })

        return result

        # ã‚¯ã‚¨ãƒªå®Ÿè¡Œå›æ•°: 2å›ï¼ˆN+1ã‚’å›é¿ï¼‰
        # N=50ã®å ´åˆ: 51å› â†’ 2å› = 96%å‰Šæ¸›

    async def get_evaluation_with_test_results(
        self,
        session: AsyncSession,
        evaluation_id: str
    ) -> Optional[dict]:
        """è©•ä¾¡ã¨ãƒ†ã‚¹ãƒˆçµæœã‚’åŠ¹ç‡çš„ã«å–å¾—ï¼ˆé›†ç´„å†…relationshipä½¿ç”¨å¯ï¼‰"""

        # é›†ç´„å†…ã¯é€šå¸¸ã®relationshipã‚’ä½¿ç”¨
        stmt = (
            select(EvaluationModel)
            .where(EvaluationModel.id == evaluation_id)
            .options(selectinload(EvaluationModel.test_results))  # Eager loading
        )

        result = await session.execute(stmt)
        evaluation = result.scalar_one_or_none()

        if not evaluation:
            return None

        return {
            "id": evaluation.id,
            "status": evaluation.status,
            "overall_score": evaluation.overall_score,
            "test_results": [
                {
                    "id": tr.id,
                    "test_case_name": tr.test_case_name,
                    "score": tr.score,
                    "passed": tr.passed,
                }
                for tr in evaluation.test_results
            ],
            "total_tests": len(evaluation.test_results),
            "passed_tests": sum(1 for tr in evaluation.test_results if tr.passed),
        }
```

#### DataLoader ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆGraphQLå¯¾å¿œæº–å‚™ï¼‰

```python
# src/infrastructure/shared/database/dataloader.py

from typing import List, Dict, Any, Callable
from collections import defaultdict

class DataLoader:
    """ãƒãƒƒãƒãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè£…ï¼ˆFacebook DataLoader ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""

    def __init__(self, batch_load_fn: Callable):
        self.batch_load_fn = batch_load_fn
        self._batch: List[Any] = []
        self._cache: Dict[Any, Any] = {}

    async def load(self, key: Any) -> Any:
        """ã‚­ãƒ¼ã«å¯¾å¿œã™ã‚‹å€¤ã‚’ãƒãƒƒãƒãƒ­ãƒ¼ãƒ‰"""
        if key in self._cache:
            return self._cache[key]

        self._batch.append(key)

        # ãƒãƒƒãƒãŒä¸€å®šã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰å®Ÿè¡Œ
        if len(self._batch) >= 100:
            await self._execute_batch()

        return self._cache.get(key)

    async def load_many(self, keys: List[Any]) -> List[Any]:
        """è¤‡æ•°ã‚­ãƒ¼ã‚’ä¸€æ‹¬ãƒ­ãƒ¼ãƒ‰"""
        self._batch.extend(keys)
        await self._execute_batch()
        return [self._cache.get(k) for k in keys]

    async def _execute_batch(self):
        """ãƒãƒƒãƒå®Ÿè¡Œ"""
        if not self._batch:
            return

        results = await self.batch_load_fn(self._batch)

        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        for key, value in zip(self._batch, results):
            self._cache[key] = value

        self._batch.clear()

# ä½¿ç”¨ä¾‹
async def batch_load_prompts(prompt_ids: List[str]) -> List[PromptModel]:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒƒãƒãƒ­ãƒ¼ãƒ‰é–¢æ•°"""
    async with get_async_session() as session:
        result = await session.execute(
            select(PromptModel).where(PromptModel.id.in_(prompt_ids))
        )
        return result.scalars().all()

prompt_loader = DataLoader(batch_load_prompts)
```

**æœŸå¾…åŠ¹æœ:**

- N+1ã‚¯ã‚¨ãƒª: **å®Œå…¨æ’é™¤**
- å¤§é‡ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚ã®ã‚¯ã‚¨ãƒªæ•°: **Nå› â†’ 2-3å›**
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ : **10-20å€é«˜é€ŸåŒ–**

---

## 6. ãƒãƒ«ã‚¯æ“ä½œã®æœ€é©åŒ–

### ç¾çŠ¶åˆ†æ

**ğŸŸ¡ åŸºæœ¬å®Ÿè£…ã®ã¿:**

```python
# tests/integration/database/test_database_connection.py (L673-679)
def test_bulk_insert_performance(self, db_session):
    prompts = [PromptModel(...) for i in range(100)]
    db_session.add_all(prompts)
    db_session.commit()
    # âœ… åŸºæœ¬çš„ãªãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
    # ğŸŸ¡ æœ€é©åŒ–ã®ä½™åœ°ã‚ã‚Š
```

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨6: Core Insert ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã¨ãƒãƒƒãƒå‡¦ç†

```python
# src/infrastructure/shared/database/bulk_operations.py

from sqlalchemy import insert, update, delete
from sqlalchemy.dialects import sqlite
from typing import List, Dict, Any

class BulkOperations:
    """é«˜é€Ÿãƒãƒ«ã‚¯æ“ä½œ"""

    @staticmethod
    async def bulk_insert_prompts(
        session: AsyncSession,
        prompts_data: List[Dict[str, Any]],
        batch_size: int = 1000
    ) -> int:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        total_inserted = 0

        # ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å®Ÿè¡Œ
        for i in range(0, len(prompts_data), batch_size):
            batch = prompts_data[i:i + batch_size]

            # Core Insertã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆï¼ˆORMä¸ä½¿ç”¨ã§é«˜é€Ÿï¼‰
            stmt = insert(PromptModel).values(batch)

            # SQLiteç”¨ã®UPSERTå¯¾å¿œ
            stmt = stmt.on_conflict_do_update(
                index_elements=['id'],
                set_=dict(
                    title=stmt.excluded.title,
                    content=stmt.excluded.content,
                    updated_at=func.now(),
                )
            )

            result = await session.execute(stmt)
            total_inserted += result.rowcount

        await session.commit()
        return total_inserted

    @staticmethod
    async def bulk_update_status(
        session: AsyncSession,
        prompt_ids: List[str],
        new_status: str
    ) -> int:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä¸€æ‹¬æ›´æ–°"""
        stmt = (
            update(PromptModel)
            .where(PromptModel.id.in_(prompt_ids))
            .values(status=new_status, updated_at=func.now())
        )

        result = await session.execute(stmt)
        await session.commit()
        return result.rowcount

    @staticmethod
    async def bulk_soft_delete(
        session: AsyncSession,
        prompt_ids: List[str]
    ) -> int:
        """è«–ç†å‰Šé™¤ä¸€æ‹¬å®Ÿè¡Œ"""
        from datetime import datetime, timezone

        stmt = (
            update(PromptModel)
            .where(PromptModel.id.in_(prompt_ids))
            .where(PromptModel.deleted_at.is_(None))
            .values(deleted_at=datetime.now(timezone.utc))
        )

        result = await session.execute(stmt)
        await session.commit()
        return result.rowcount

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
@pytest.mark.performance
async def test_bulk_operations_performance(async_session):
    """ãƒãƒ«ã‚¯æ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    import time

    # 10,000ä»¶ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿
    prompts_data = [
        {
            "id": str(uuid.uuid4()),
            "title": f"Bulk Prompt {i}",
            "content": f"Content {i}",
            "user_id": "test_user",
            "status": "draft",
            "version": 1,
        }
        for i in range(10000)
    ]

    start = time.perf_counter()
    inserted = await BulkOperations.bulk_insert_prompts(
        async_session,
        prompts_data
    )
    elapsed = time.perf_counter() - start

    assert inserted == 10000
    # ç›®æ¨™: 10,000ä»¶ã‚’5ç§’ä»¥å†…
    assert elapsed < 5.0, f"Bulk insert too slow: {elapsed:.3f}s"

    print(f"âœ… Inserted {inserted} records in {elapsed:.3f}s")
    print(f"ğŸ“Š Throughput: {inserted/elapsed:.0f} records/sec")
```

**æœŸå¾…åŠ¹æœ:**

- ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆé€Ÿåº¦: **5-10å€é«˜é€ŸåŒ–**
- 10,000ä»¶å‡¦ç†æ™‚é–“: **30ç§’ â†’ 5ç§’**
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: **50%å‰Šæ¸›**

---

## 7. ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†ã®æœ€é©åŒ–

### ç¾çŠ¶åˆ†æ

**ğŸŸ¡ åŸºæœ¬å®Ÿè£…ã®ã¿:**

```python
# turso_connection.py (L85-92)
def get_db_session() -> Session:
    session = _turso_connection.get_session()
    try:
        yield session
    finally:
        session.close()
    # ğŸ”´ ã‚³ãƒŸãƒƒãƒˆ/ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ãŒä¸æ˜ç­
```

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨7: SAVEPOINTã¨ãƒã‚¹ãƒˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³

```python
# src/infrastructure/shared/database/transaction_manager.py

from contextlib import asynccontextmanager
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import IntegrityError, OperationalError
import logging

logger = logging.getLogger(__name__)

class TransactionManager:
    """ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    @staticmethod
    @asynccontextmanager
    async def transaction(
        session: AsyncSession,
        use_savepoint: bool = False
    ):
        """ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
        if use_savepoint:
            # ãƒã‚¹ãƒˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆSAVEPOINTä½¿ç”¨ï¼‰
            async with session.begin_nested():
                try:
                    yield session
                except Exception as e:
                    logger.error(f"Nested transaction failed: {e}")
                    raise
        else:
            # é€šå¸¸ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³
            try:
                yield session
                await session.commit()
            except IntegrityError as e:
                await session.rollback()
                logger.error(f"Integrity error: {e}")
                raise
            except OperationalError as e:
                await session.rollback()
                logger.error(f"Operational error: {e}")
                raise
            except Exception as e:
                await session.rollback()
                logger.error(f"Unexpected error: {e}")
                raise

    @staticmethod
    @asynccontextmanager
    async def read_only_transaction(session: AsyncSession):
        """èª­ã¿å–ã‚Šå°‚ç”¨ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæœ€é©åŒ–ï¼‰"""
        # èª­ã¿å–ã‚Šå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã§ãƒ­ãƒƒã‚¯ã‚’æœ€å°åŒ–
        await session.execute(text("PRAGMA query_only = ON"))
        try:
            yield session
        finally:
            await session.execute(text("PRAGMA query_only = OFF"))

# ä½¿ç”¨ä¾‹: è¤‡é›‘ãªãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³
async def create_prompt_with_evaluation(
    session: AsyncSession,
    prompt_data: dict,
    evaluation_data: dict
) -> tuple[PromptModel, EvaluationModel]:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨è©•ä¾¡ã‚’åŸå­çš„ã«ä½œæˆ"""

    async with TransactionManager.transaction(session):
        # Step 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = PromptModel(**prompt_data)
        session.add(prompt)
        await session.flush()  # IDã‚’å–å¾—

        # Step 2: è©•ä¾¡ä½œæˆï¼ˆãƒã‚¹ãƒˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
        async with TransactionManager.transaction(session, use_savepoint=True):
            evaluation = EvaluationModel(
                **evaluation_data,
                prompt_id=prompt.id
            )
            session.add(evaluation)
            await session.flush()

        # ä¸¡æ–¹æˆåŠŸæ™‚ã®ã¿ã‚³ãƒŸãƒƒãƒˆ
        return prompt, evaluation

# ä½¿ç”¨ä¾‹: èª­ã¿å–ã‚Šå°‚ç”¨ã‚¯ã‚¨ãƒª
async def get_statistics(session: AsyncSession) -> dict:
    """çµ±è¨ˆæƒ…å ±å–å¾—ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨æœ€é©åŒ–ï¼‰"""

    async with TransactionManager.read_only_transaction(session):
        total_prompts = await session.scalar(
            select(func.count(PromptModel.id))
        )

        total_evaluations = await session.scalar(
            select(func.count(EvaluationModel.id))
        )

        return {
            "total_prompts": total_prompts,
            "total_evaluations": total_evaluations,
        }
```

**æœŸå¾…åŠ¹æœ:**

- ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³æ•´åˆæ€§: **100%ä¿è¨¼**
- ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯ç™ºç”Ÿ: **90%å‰Šæ¸›**
- èª­ã¿å–ã‚Šã‚¯ã‚¨ãƒªãƒ­ãƒƒã‚¯: **å®Œå…¨æ’é™¤**

---

## 8. ãƒ¡ãƒ¢ãƒªç®¡ç†ã®æœ€é©åŒ–

### ç¾çŠ¶åˆ†æ

**âšª æ¸¬å®šæœªå®Ÿæ–½:**

- ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ä¸è¶³
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰æ™‚ã®æŒ™å‹•ä¸æ˜

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨8: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¨ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³

```python
# src/infrastructure/shared/database/streaming.py

from typing import AsyncGenerator
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

class StreamingQuery:
    """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†"""

    @staticmethod
    async def stream_prompts(
        session: AsyncSession,
        user_id: str,
        chunk_size: int = 1000
    ) -> AsyncGenerator[list[PromptModel], None]:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å–å¾—"""
        offset = 0

        while True:
            stmt = (
                select(PromptModel)
                .where(PromptModel.user_id == user_id)
                .where(PromptModel.deleted_at.is_(None))
                .order_by(PromptModel.created_at.desc())
                .offset(offset)
                .limit(chunk_size)
            )

            result = await session.execute(stmt)
            prompts = result.scalars().all()

            if not prompts:
                break

            yield prompts

            offset += chunk_size

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢ã—ã¦ãƒ¡ãƒ¢ãƒªè§£æ”¾
            session.expunge_all()

    @staticmethod
    async def cursor_based_pagination(
        session: AsyncSession,
        user_id: str,
        cursor: Optional[str] = None,
        limit: int = 50
    ) -> dict:
        """ã‚«ãƒ¼ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰"""
        stmt = (
            select(PromptModel)
            .where(PromptModel.user_id == user_id)
            .where(PromptModel.deleted_at.is_(None))
        )

        if cursor:
            # ã‚«ãƒ¼ã‚½ãƒ«ï¼ˆæœ€å¾Œã®created_atï¼‰ã‚ˆã‚Šå¾Œã®ãƒ‡ãƒ¼ã‚¿
            stmt = stmt.where(PromptModel.created_at < cursor)

        stmt = stmt.order_by(PromptModel.created_at.desc()).limit(limit + 1)

        result = await session.execute(stmt)
        prompts = result.scalars().all()

        has_next = len(prompts) > limit
        if has_next:
            prompts = prompts[:limit]

        next_cursor = prompts[-1].created_at if prompts and has_next else None

        return {
            "data": prompts,
            "next_cursor": next_cursor,
            "has_next": has_next,
        }

# ä½¿ç”¨ä¾‹: å¤§é‡ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
async def export_all_prompts(user_id: str) -> str:
    """å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰"""
    import csv
    from io import StringIO

    output = StringIO()
    writer = csv.writer(output)
    writer.writerow(['ID', 'Title', 'Content', 'Created At'])

    async with get_async_session() as session:
        async for chunk in StreamingQuery.stream_prompts(session, user_id):
            for prompt in chunk:
                writer.writerow([
                    prompt.id,
                    prompt.title,
                    prompt.content,
                    prompt.created_at,
                ])

    return output.getvalue()
```

**æœŸå¾…åŠ¹æœ:**

- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: **70%å‰Šæ¸›**
- å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†: **OOMã‚¨ãƒ©ãƒ¼å®Œå…¨æ’é™¤**
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†é€Ÿåº¦: **ä¸€å®šãƒ¡ãƒ¢ãƒªã§ç„¡åˆ¶é™ãƒ‡ãƒ¼ã‚¿å‡¦ç†**

---

## 9. Turso ã‚¨ãƒƒã‚¸ãƒ‡ãƒ—ãƒ­ã‚¤æœ€é©åŒ–

### ç¾çŠ¶åˆ†æ

**ğŸŸ¡ åŸºæœ¬è¨­å®šã®ã¿:**

```python
# turso_connection.py (L22-44)
def get_connection_url(self) -> str:
    env = os.getenv("APP_ENV", "local")
    if env == "production":
        url = os.getenv("TURSO_DATABASE_URL")
        # âœ… ç’°å¢ƒåˆ¥æ¥ç¶š
        # ğŸŸ¡ ã‚¨ãƒƒã‚¸æœ€é©åŒ–ãªã—
```

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨9: ã‚¨ãƒƒã‚¸ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–

```python
# src/infrastructure/shared/database/turso_edge_router.py

import os
from typing import Optional
from geolite2 import geolite2
import httpx

class TursoEdgeRouter:
    """Tursoã‚¨ãƒƒã‚¸ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""

    # Tursoã‚¨ãƒƒã‚¸ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ2025å¹´ç¾åœ¨ï¼‰
    EDGE_LOCATIONS = {
        "us-east": "libsql://autoforge-us-east.turso.io",
        "us-west": "libsql://autoforge-us-west.turso.io",
        "eu-west": "libsql://autoforge-eu-west.turso.io",
        "ap-northeast": "libsql://autoforge-ap-northeast.turso.io",
    }

    @staticmethod
    def get_client_location(ip_address: str) -> str:
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆIPã‹ã‚‰æœ€é©ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š"""
        reader = geolite2.reader()
        match = reader.get(ip_address)

        if not match:
            return "us-east"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        continent = match.get('continent', {}).get('code')
        country = match.get('country', {}).get('iso_code')

        # ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        if continent == 'AS':
            return "ap-northeast"
        elif continent == 'EU':
            return "eu-west"
        elif country == 'US':
            # ç±³å›½å†…ã§ã•ã‚‰ã«æ±è¥¿åˆ¤å®š
            longitude = match.get('location', {}).get('longitude', -100)
            return "us-east" if longitude > -100 else "us-west"
        else:
            return "us-east"

    @staticmethod
    async def get_optimal_connection_url(
        client_ip: Optional[str] = None
    ) -> str:
        """æœ€é©ãªTursoæ¥ç¶šURLã‚’è¿”ã™"""
        if not client_ip:
            # Cloudflare WorkersçµŒç”±ã®å ´åˆã€CFãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰å–å¾—
            client_ip = os.getenv("CF_CONNECTING_IP")

        if client_ip:
            location = TursoEdgeRouter.get_client_location(client_ip)
            base_url = TursoEdgeRouter.EDGE_LOCATIONS.get(location)
        else:
            base_url = TursoEdgeRouter.EDGE_LOCATIONS["us-east"]

        token = os.getenv("TURSO_AUTH_TOKEN")
        return f"{base_url}?authToken={token}"

    @staticmethod
    async def health_check_all_edges() -> dict[str, bool]:
        """å…¨ã‚¨ãƒƒã‚¸ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        results = {}

        async with httpx.AsyncClient() as client:
            for location, url in TursoEdgeRouter.EDGE_LOCATIONS.items():
                try:
                    response = await client.get(
                        f"https://{url.split('//')[1]}/health",
                        timeout=5.0
                    )
                    results[location] = response.status_code == 200
                except Exception:
                    results[location] = False

        return results

# FastAPIçµ±åˆ
from fastapi import Request

async def get_optimized_db_session(request: Request):
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«æœ€é©åŒ–ã•ã‚ŒãŸDBæ¥ç¶š"""
    client_ip = request.headers.get("CF-Connecting-IP") or request.client.host

    connection_url = await TursoEdgeRouter.get_optimal_connection_url(client_ip)

    # å‹•çš„ã«æ¥ç¶šå…ˆã‚’å¤‰æ›´
    engine = create_async_engine(connection_url, pool_size=5)
    async_session = async_sessionmaker(engine, expire_on_commit=False)

    async with async_session() as session:
        yield session
```

#### ã‚¨ãƒƒã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

```python
# Cloudflare Workersçµ±åˆ
# workers/database-proxy.py

from js import Response, fetch

async def on_fetch(request):
    """Cloudflare Workersã§ã®DBæ“ä½œãƒ—ãƒ­ã‚­ã‚·"""

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å–å¾—
    cf = request.cf
    client_country = cf.get('country', 'US')
    client_colo = cf.get('colo', 'SJC')  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã‚³ãƒ¼ãƒ‰

    # æœ€å¯„ã‚Šã®Tursoã‚¨ãƒƒã‚¸ã«è»¢é€
    optimal_edge = get_nearest_turso_edge(client_country, client_colo)

    # ã‚¨ãƒƒã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    cache = caches.default
    cache_key = f"turso:{request.url}"
    cached_response = await cache.match(cache_key)

    if cached_response:
        return cached_response

    # Tursoã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    response = await fetch(optimal_edge, {
        'method': request.method,
        'headers': request.headers,
        'body': request.body,
    })

    # èª­ã¿å–ã‚Šã‚¯ã‚¨ãƒªã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    if request.method == 'GET':
        await cache.put(cache_key, response.clone(), {
            'expirationTtl': 300  # 5åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        })

    return response
```

**æœŸå¾…åŠ¹æœ:**

- ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: **å¹³å‡100ms â†’ 20ms** (80%æ”¹å–„)
- ã‚¢ã‚¸ã‚¢-ç±³å›½é–“: **300ms â†’ 50ms** (83%æ”¹å–„)
- ã‚¨ãƒƒã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: **70%ä»¥ä¸Š**

---

## 10. ç›£è¦–ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### ç¾çŠ¶åˆ†æ

**ğŸ”´ æœªå®Ÿè£…:**

- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãªã—
- ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªæ¤œå‡ºãªã—
- ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šãŒå›°é›£

### æœ€é©åŒ–æ¨å¥¨

#### æ¨å¥¨10: åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

```python
# src/core/monitoring/database_metrics.py

from prometheus_client import Counter, Histogram, Gauge
import time
from functools import wraps

# Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
db_query_duration = Histogram(
    'db_query_duration_seconds',
    'Database query duration',
    ['operation', 'table']
)

db_query_total = Counter(
    'db_query_total',
    'Total database queries',
    ['operation', 'table', 'status']
)

db_connection_pool_size = Gauge(
    'db_connection_pool_size',
    'Current connection pool size'
)

db_connection_pool_overflow = Gauge(
    'db_connection_pool_overflow',
    'Current connection pool overflow'
)

# ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãƒ­ã‚°
slow_query_log = []

def monitor_query(operation: str, table: str, threshold_ms: float = 100):
    """ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start = time.perf_counter()
            status = "success"

            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                status = "error"
                raise
            finally:
                duration = time.perf_counter() - start

                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                db_query_duration.labels(
                    operation=operation,
                    table=table
                ).observe(duration)

                db_query_total.labels(
                    operation=operation,
                    table=table,
                    status=status
                ).inc()

                # ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªæ¤œå‡º
                if duration * 1000 > threshold_ms:
                    slow_query_log.append({
                        "operation": operation,
                        "table": table,
                        "duration_ms": duration * 1000,
                        "timestamp": time.time(),
                        "function": func.__name__,
                    })

                    logger.warning(
                        f"Slow query detected: {operation} on {table} "
                        f"took {duration*1000:.2f}ms"
                    )

        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹
class MonitoredPromptRepository:
    """ç›£è¦–ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªãƒã‚¸ãƒˆãƒª"""

    @monitor_query(operation="select", table="prompts")
    async def find_by_id(
        self,
        session: AsyncSession,
        prompt_id: str
    ) -> Optional[PromptModel]:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå–å¾—ï¼ˆç›£è¦–ä»˜ãï¼‰"""
        result = await session.execute(
            select(PromptModel).where(PromptModel.id == prompt_id)
        )
        return result.scalar_one_or_none()

    @monitor_query(operation="insert", table="prompts")
    async def create(
        self,
        session: AsyncSession,
        prompt: PromptModel
    ) -> PromptModel:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆç›£è¦–ä»˜ãï¼‰"""
        session.add(prompt)
        await session.flush()
        return prompt

# æ¥ç¶šãƒ—ãƒ¼ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
def collect_pool_metrics(engine):
    """æ¥ç¶šãƒ—ãƒ¼ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šæœŸåé›†"""
    pool = engine.pool
    db_connection_pool_size.set(pool.size())
    db_connection_pool_overflow.set(pool.overflow())

# ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
def generate_slow_query_report() -> str:
    """ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãƒ¬ãƒãƒ¼ãƒˆ"""
    if not slow_query_log:
        return "No slow queries detected"

    from collections import defaultdict

    # é›†è¨ˆ
    by_table = defaultdict(list)
    for entry in slow_query_log:
        by_table[entry['table']].append(entry)

    report = "=== Slow Query Report ===\n\n"

    for table, queries in sorted(
        by_table.items(),
        key=lambda x: len(x[1]),
        reverse=True
    ):
        report += f"Table: {table} ({len(queries)} slow queries)\n"

        # å¹³å‡æ™‚é–“
        avg_duration = sum(q['duration_ms'] for q in queries) / len(queries)
        report += f"  Average duration: {avg_duration:.2f}ms\n"

        # æœ€é…ã‚¯ã‚¨ãƒª
        slowest = max(queries, key=lambda x: x['duration_ms'])
        report += f"  Slowest: {slowest['duration_ms']:.2f}ms "
        report += f"({slowest['operation']})\n\n"

    return report
```

#### Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š

```yaml
# monitoring/grafana/dashboards/database_performance.json

{
  'dashboard':
    {
      'title': 'Database Performance',
      'panels':
        [
          {
            'title': 'Query Duration (P95)',
            'targets':
              [
                {
                  'expr': 'histogram_quantile(0.95, db_query_duration_seconds)',
                },
              ],
          },
          {
            'title': 'Queries per Second',
            'targets': [{ 'expr': 'rate(db_query_total[5m])' }],
          },
          {
            'title': 'Connection Pool Usage',
            'targets':
              [
                {
                  'expr':
                    'db_connection_pool_size / db_connection_pool_max * 100',
                },
              ],
          },
          {
            'title': 'Slow Queries (>100ms)',
            'targets': [{ 'expr': 'db_query_duration_seconds > 0.1' }],
          },
        ],
    },
}
```

**æœŸå¾…åŠ¹æœ:**

- ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šæ™‚é–“: **æ•°æ—¥ â†’ æ•°åˆ†**
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–æ¤œçŸ¥: **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ **
- æœ€é©åŒ–åŠ¹æœæ¸¬å®š: **å®šé‡çš„è©•ä¾¡å¯èƒ½**

---

## å®Ÿè£…å„ªå…ˆé †ä½ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| æœ€é©åŒ–é …ç›®              | å½±éŸ¿åº¦  | å®Ÿè£…é›£æ˜“åº¦ | æœŸå¾…ROI | å„ªå…ˆåº¦ |
| ----------------------- | ------- | ---------- | ------- | ------ |
| 3. Redisã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°    | ğŸ”´ æ¥µé«˜ | ğŸŸ¢ ä½      | 10x     | **P0** |
| 2. éåŒæœŸã‚¯ã‚¨ãƒªæ‹¡å¼µ     | ğŸ”´ æ¥µé«˜ | ğŸŸ¡ ä¸­      | 5-10x   | **P0** |
| 5. N+1å•é¡Œæ’é™¤          | ğŸ”´ é«˜   | ğŸŸ¢ ä½      | 10-20x  | **P0** |
| 1. æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–     | ğŸŸ¡ ä¸­   | ğŸŸ¢ ä½      | 2x      | **P1** |
| 4. è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹     | ğŸŸ¡ ä¸­   | ğŸŸ¢ ä½      | 5-10x   | **P1** |
| 10. ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹      | ğŸŸ¡ ä¸­   | ğŸŸ¡ ä¸­      | ç¶™ç¶šçš„  | **P1** |
| 6. ãƒãƒ«ã‚¯æ“ä½œæœ€é©åŒ–     | ğŸŸ¡ ä¸­   | ğŸŸ¢ ä½      | 5-10x   | **P2** |
| 7. ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç† | ğŸŸ¢ ä½   | ğŸŸ¡ ä¸­      | 2x      | **P2** |
| 8. ãƒ¡ãƒ¢ãƒªç®¡ç†           | ğŸŸ¢ ä½   | ğŸŸ¡ ä¸­      | 2-3x    | **P2** |
| 9. Tursoã‚¨ãƒƒã‚¸æœ€é©åŒ–    | ğŸŸ¢ ä½   | ğŸ”´ é«˜      | 5x      | **P3** |

### æ¨å¥¨å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

#### Week 1-2: P0æœ€é©åŒ–ï¼ˆMVPå¿…é ˆï¼‰

1. **Redisã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°å®Ÿè£…** (3æ—¥)

   - åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±¤å®Ÿè£…
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè©³ç´°ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
   - ç„¡åŠ¹åŒ–æˆ¦ç•¥å®Ÿè£…

2. **éåŒæœŸã‚¯ã‚¨ãƒªæ‹¡å¼µ** (4æ—¥)

   - AsyncTursoConnectionå®Ÿè£…
   - ä¸»è¦ãƒªãƒã‚¸ãƒˆãƒªã®éåŒæœŸåŒ–
   - ãƒ†ã‚¹ãƒˆè¿½åŠ 

3. **N+1å•é¡Œæ’é™¤** (3æ—¥)
   - Eager loadingå®Ÿè£…
   - DataLoaderãƒ‘ã‚¿ãƒ¼ãƒ³å°å…¥
   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

#### Week 3: P1æœ€é©åŒ–

4. **æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–** (2æ—¥)
5. **è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ ** (1æ—¥)
6. **ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…** (2æ—¥)

#### Week 4+: P2-P3æœ€é©åŒ–

7. æ®‹ã‚Šã®æœ€é©åŒ–é …ç›®ã‚’æ®µéšçš„ã«å®Ÿè£…

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœï¼ˆäºˆæ¸¬ï¼‰

### ç¾çŠ¶ï¼ˆPhase 4åŸºæœ¬å®Ÿè£…ï¼‰

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹         | ç¾çŠ¶  |
| ------------------ | ----- |
| API P50ãƒ¬ã‚¹ãƒãƒ³ã‚¹  | 200ms |
| API P95ãƒ¬ã‚¹ãƒãƒ³ã‚¹  | 500ms |
| DBã‚¯ã‚¨ãƒªå¹³å‡       | 80ms  |
| åŒæ™‚æ¥ç¶šä¸Šé™       | 100   |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ | 0%    |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡       | 512MB |

### æœ€é©åŒ–å¾Œï¼ˆå…¨æ¨å¥¨å®Ÿè£…å®Œäº†æ™‚ï¼‰

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹         | ç›®æ¨™      | æ”¹å–„ç‡      |
| ------------------ | --------- | ----------- |
| API P50ãƒ¬ã‚¹ãƒãƒ³ã‚¹  | **20ms**  | **90%æ”¹å–„** |
| API P95ãƒ¬ã‚¹ãƒãƒ³ã‚¹  | **150ms** | **70%æ”¹å–„** |
| DBã‚¯ã‚¨ãƒªå¹³å‡       | **30ms**  | **62%æ”¹å–„** |
| åŒæ™‚æ¥ç¶šä¸Šé™       | **1000+** | **10å€**    |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ | **80%**   | **æ–°è¦**    |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡       | **300MB** | **41%å‰Šæ¸›** |

---

## æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 

### å³åº§ã«å®Ÿæ–½ã™ã¹ãé …ç›®ï¼ˆä»Šé€±ï¼‰

1. âœ… **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆåŸºç›¤æ§‹ç¯‰**

   - pytest-benchmarkå°å…¥
   - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸¬å®š

2. âœ… **Redisã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°å®Ÿè£…é–‹å§‹**

   - redis-pyå°å…¥
   - åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼å®Ÿè£…

3. âœ… **éåŒæœŸæ¥ç¶šã®è¨­è¨ˆãƒ¬ãƒ“ãƒ¥ãƒ¼**
   - SQLAlchemy 2.0 asyncå¯¾å¿œç¢ºèª
   - libSQL asyncäº’æ›æ€§èª¿æŸ»

### æ¥é€±ä»¥é™ã®è¨ˆç”»

4. è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
5. N+1å•é¡Œå¯¾ç­–å®Ÿè£…
6. ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰

---

## å‚è€ƒè³‡æ–™

### æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [SQLAlchemy 2.0 Async Documentation](https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html)
- [Turso Performance Best Practices](https://docs.turso.tech/performance)
- [Redis Caching Patterns](https://redis.io/docs/manual/patterns/)
- [libSQL Vector Extension Guide](https://docs.turso.tech/features/vector-search)

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

- [Web Performance Budget Calculator](https://www.performancebudget.io/)
- [Database Performance Monitoring Guide](https://www.datadoghq.com/knowledge-center/database-performance-monitoring/)

---

## çµè«–

Phase
4ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®Ÿè£…ã¯**å …å®ŸãªåŸºç›¤**ã‚’æä¾›ã—ã¦ã„ã¾ã™ãŒã€æœ¬ç•ªç’°å¢ƒã§ã®é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã€**10ã®é‡è¦ãªæœ€é©åŒ–**ã‚’å®Ÿæ–½ã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚

**æœ€å„ªå…ˆäº‹é …ï¼ˆP0ï¼‰:**

1. Redisã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°å®Ÿè£… â†’ **APIå¿œç­”10å€é«˜é€ŸåŒ–**
2. éåŒæœŸã‚¯ã‚¨ãƒªæ‹¡å¼µ â†’ **ä¸¦åˆ—å‡¦ç†5-10å€é«˜é€ŸåŒ–**
3. N+1å•é¡Œå®Œå…¨æ’é™¤ â†’ **é–¢é€£ãƒ‡ãƒ¼ã‚¿å–å¾—10-20å€é«˜é€ŸåŒ–**

ã“ã‚Œã‚‰ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€AutoForgeNexusã¯**ä¸–ç•Œä¸­ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ã€é«˜é€Ÿã§å®‰å®šã—ãŸAIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ä½“é¨“ã‚’æä¾›**ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

---

**ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†æ—¥**: 2025å¹´10æœˆ1æ—¥ **æ¬¡å›ãƒ¬ãƒ“ãƒ¥ãƒ¼äºˆå®š**: æœ€é©åŒ–å®Ÿè£…å¾Œï¼ˆ2é€±é–“å¾Œï¼‰
