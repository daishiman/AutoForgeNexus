# AutoForgeNexus è¦³æ¸¬æ€§ï¼ˆObservabilityï¼‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ

**ãƒ¬ãƒ“ãƒ¥ãƒ¼æ—¥**: 2025å¹´10æœˆ8æ—¥ **å¯¾è±¡ç¯„å›²**: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¦³æ¸¬æ€§å®Ÿè£…ï¼ˆPhase
3é€²æ—ç¢ºèªï¼‰ **ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼**: observability-engineer Agentï¼ˆPierre Vincent, Cindy
Sridharan, Yuri Shkuroï¼‰ **é‡è¦åº¦**: ğŸ”´
Criticalï¼ˆæœ¬ç•ªé‹ç”¨ã®æˆå¦ã‚’å·¦å³ã™ã‚‹åŸºç›¤æ©Ÿèƒ½ï¼‰

---

## ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ç·åˆè©•ä¾¡: â­â­â­â­â˜† (4.2/5.0)

AutoForgeNexusã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¦³æ¸¬æ€§å®Ÿè£…ã¯ã€**LLMç‰¹åŒ–ã‚·ã‚¹ãƒ†ãƒ ã«å¿…è¦ãªè¦³æ¸¬å¯èƒ½æ€§ã®åŸºç¤ã‚’ç¢ºç«‹**ã—ã¦ãŠã‚Šã€é«˜ã„æ°´æº–ã«é”ã—ã¦ã„ã¾ã™ã€‚æ§‹é€ åŒ–ãƒ­ã‚°ã€åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®ä¸‰æœ¬æŸ±ã‚’å®Ÿè£…ã—ã€LangFuseã«ã‚ˆã‚‹LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ã‚’æº–å‚™ã—ã¦ã„ã¾ã™ã€‚

**ä¸»è¦ãªå¼·ã¿**:

- ğŸ¯ æ§‹é€ åŒ–ãƒ­ã‚°ã®ä¸€è²«æ€§ï¼ˆJSONå½¢å¼ã€TypedDictå‹å®‰å…¨æ€§ï¼‰
- ğŸ” ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¿½è·¡ï¼ˆrequest_idã€call_idã€query_idï¼‰
- ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é…æ…®ï¼ˆæ©Ÿå¯†æƒ…å ±ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€DoSå¯¾ç­–ï¼‰
- ğŸ“Š åŒ…æ‹¬çš„ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã€ä¾å­˜é–¢ä¿‚ã€SLIå¯¾å¿œæº–å‚™ï¼‰

**æ”¹å–„ãŒå¿…è¦ãªé ˜åŸŸ**:

- âš ï¸ OpenTelemetryçµ±åˆæœªå®Ÿè£…ï¼ˆæ¥­ç•Œæ¨™æº–ã¸ã®æº–æ‹ å¿…é ˆï¼‰
- âš ï¸ LangFuseå®Ÿè£…æœªå®Œæˆï¼ˆLLMãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ä¸å®Œå…¨ï¼‰
- âš ï¸ äºˆæ¸¬çš„ã‚¢ãƒ©ãƒ¼ãƒˆä¸è¶³ï¼ˆç•°å¸¸æ¤œçŸ¥ã€ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆæœªå®Ÿè£…ï¼‰
- âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ä½™åœ°ï¼ˆãƒ­ã‚°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã€éåŒæœŸåŒ–ï¼‰

### ã‚¹ã‚³ã‚¢å†…è¨³

| è¦³ç‚¹                   | ã‚¹ã‚³ã‚¢ | è©•ä¾¡                              |
| ---------------------- | ------ | --------------------------------- |
| åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å®Ÿè£…   | 3.5/5  | åŸºç¤ã¯å®Œæˆã€OpenTelemetryçµ±åˆå¾…ã¡ |
| ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†         | 4.0/5  | åŒ…æ‹¬çš„ã ãŒé›†ç´„ãƒ»å¯è¦–åŒ–ãŒæœªå®Œæˆ    |
| æ§‹é€ åŒ–ãƒ­ã‚°             | 4.5/5  | é«˜å“è³ªã€å‹å®‰å…¨ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾å¿œ  |
| LangFuseçµ±åˆ           | 3.0/5  | æº–å‚™å®Œäº†ã ãŒå®Ÿè£…æœªå®Œæˆ            |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–     | 3.8/5  | ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªæ¤œçŸ¥ã‚ã‚Šã€SLOæœªå®Ÿè£…   |
| ã‚¨ãƒ©ãƒ¼è¿½è·¡             | 4.2/5  | è©³ç´°ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€å†ç™ºç”Ÿå¯èƒ½    |
| ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š           | 2.5/5  | åŸºæœ¬ãƒ­ã‚°ã®ã¿ã€äºˆæ¸¬çš„æ©Ÿèƒ½ä¸è¶³      |
| ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹  | 4.0/5  | 2025å¹´æ¨™æº–ã«æ¦‚ã­é©åˆ              |

---

## 1ï¸âƒ£ åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å®Ÿè£…ã®é©åˆ‡æ€§

### âœ… å¼·ã¿

#### 1.1 ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¿½è·¡ã®å®Ÿè£…

```python
# observability.py:130
request_id = str(uuid.uuid4())
context: RequestContext = {
    "request_id": request_id,
    "timestamp": datetime.now(UTC).isoformat(),
    ...
}
```

**è©•ä¾¡**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆå…¨ä½“ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ä¼æ’­ã—ã€ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒˆãƒ¬ãƒ¼ã‚¹ã®ç›¸é–¢åˆ†æãŒå¯èƒ½ã€‚

#### 1.2 LLMå‘¼ã³å‡ºã—ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

```python
# observability.py:342-363
async def track_llm_call(
    self, provider: str, model: str, prompt: str,
    user_id: str | None = None, session_id: str | None = None
) -> AsyncGenerator[str, None]:
    call_id = str(uuid.uuid4())
    context: LLMCallContext = {...}
```

**è©•ä¾¡**:
LLMå›ºæœ‰ã®è¦³æ¸¬ãƒ‹ãƒ¼ã‚ºã«å¯¾å¿œã—ã€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–¢é€£ä»˜ã‘ã€‚

#### 1.3 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

```python
# observability.py:418-444
async def track_query(
    self, operation: str, table: str | None = None,
    user_id: str | None = None
) -> AsyncGenerator[str, None]:
    ...
    # ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªã®è­¦å‘Š
    if duration > 1.0:  # 1ç§’ä»¥ä¸Š
        self.logger.warning("Slow database query detected", ...)
```

**è©•ä¾¡**: 1ç§’é–¾å€¤ã§ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªæ¤œçŸ¥ã€ç¶™ç¶šçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãŒå¯èƒ½ã€‚

### âš ï¸ æ”¹å–„ãŒå¿…è¦ãªé …ç›®

#### 1.4 OpenTelemetryçµ±åˆã®æ¬ å¦‚ï¼ˆCriticalï¼‰

**å•é¡Œ**: æ¥­ç•Œæ¨™æº–ã®OpenTelemetryï¼ˆOTelï¼‰SDKã‚’ä½¿ç”¨ã—ã¦ã„ãªã„ã€‚

**å½±éŸ¿**:

- Jaegerã€Zipkinã€Tempoãªã©ã®åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰çµ±åˆãŒå›°é›£
- ã‚¹ãƒ‘ãƒ³éšå±¤ã®å¯è¦–åŒ–ã€ã‚µãƒ¼ãƒ“ã‚¹ãƒãƒƒãƒ—ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åˆ†æã®è‡ªå‹•åŒ–ä¸å¯
- OpenLLMetryã€OpenLITã¨ã®äº’æ›æ€§ãªã—

**æ¨å¥¨å®Ÿè£…**:

```python
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# FastAPIè‡ªå‹•è¨ˆæ¸¬
FastAPIInstrumentor.instrument_app(app)

# LangFuseç”¨OTLPã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼
otlp_exporter = OTLPSpanExporter(
    endpoint="https://cloud.langfuse.com/api/public/ingestion",
    headers={
        "Authorization": f"Bearer {settings.langfuse_secret_key}"
    }
)
```

**å‚è€ƒ**:
[LangFuse OTel Native SDK](https://langfuse.com/docs/integrations/opentelemetry)
(2025å¹´æœ€æ–°)

#### 1.5 GenAIã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¦ç´„æœªæº–æ‹ 

**å•é¡Œ**: OpenTelemetry GenAIä»•æ§˜ã«æº–æ‹ ã—ã¦ã„ãªã„ã€‚

**æ¨å¥¨å±æ€§**:

```python
span.set_attribute("gen_ai.system", "openai")  # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
span.set_attribute("gen_ai.request.model", "gpt-4")
span.set_attribute("gen_ai.request.max_tokens", 1024)
span.set_attribute("gen_ai.request.temperature", 0.7)
span.set_attribute("gen_ai.response.finish_reasons", ["stop"])
span.set_attribute("gen_ai.usage.prompt_tokens", 150)
span.set_attribute("gen_ai.usage.completion_tokens", 300)
```

**å‚è€ƒ**:
[OTel Semantic Conventions for GenAI](https://opentelemetry.io/docs/specs/semconv/gen-ai/)

#### 1.6 åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®ä¼æ’­ä¸è¶³

**å•é¡Œ**:
`turso_connection.py`ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒä¼æ’­ã•ã‚Œã¦ã„ãªã„ã€‚

```python
# turso_connection.py:121-128ï¼ˆç¾çŠ¶ï¼‰
async def execute_raw(
    self,
    query: str,
    params: dict[str, str | int | float | bool | None] | None = None
) -> ResultSet:
    client = self.get_libsql_client()
    return await client.execute(query, params or {})
```

**æ¨å¥¨å®Ÿè£…**:

```python
from opentelemetry import trace

async def execute_raw(
    self,
    query: str,
    params: dict[str, str | int | float | bool | None] | None = None
) -> ResultSet:
    tracer = trace.get_tracer(__name__)
    with tracer.start_as_current_span("db.execute") as span:
        span.set_attribute("db.system", "turso")
        span.set_attribute("db.statement", query[:100])  # æœ€åˆã®100æ–‡å­—ã®ã¿
        span.set_attribute("db.operation", self._extract_operation(query))

        client = self.get_libsql_client()
        result = await client.execute(query, params or {})

        span.set_attribute("db.rows_affected", len(result.rows))
        return result
```

---

## 2ï¸âƒ£ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®ç¶²ç¾…æ€§

### âœ… å¼·ã¿

#### 2.1 ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# monitoring.py:407-423
def record_request_metrics(
    self, method: str, endpoint: str, status_code: int, duration: float
) -> None:
    metric = {
        "timestamp": timestamp,
        "type": "http_request",
        "method": method,
        "endpoint": endpoint,
        "status_code": status_code,
        "duration_ms": duration * 1000,
        "environment": os.getenv("ENVIRONMENT", "development"),
    }
```

**è©•ä¾¡**: HTTPåŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¶²ç¾…ã—ã€ç’°å¢ƒåˆ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒå¯èƒ½ã€‚

#### 2.2 ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# monitoring.py:137-169
def _get_system_metrics(self) -> SystemMetrics:
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage("/")
    load_average = list(os.getloadavg())
    process_count = len(psutil.pids())
```

**è©•ä¾¡**: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å¥å…¨æ€§ã‚’åŒ…æ‹¬çš„ã«ç›£è¦–ã€‚

### âš ï¸ æ”¹å–„ãŒå¿…è¦ãªé …ç›®

#### 2.3 Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªå®Ÿè£…ï¼ˆCriticalï¼‰

**å•é¡Œ**: Prometheusãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãŒãªã„ã€‚

**å½±éŸ¿**:

- Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆä¸å¯
- ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šï¼ˆAlertmanagerï¼‰ä¸å¯
- æ™‚ç³»åˆ—åˆ†æã€å‚¾å‘åˆ†æã€å®¹é‡è¨ˆç”»å›°é›£

**æ¨å¥¨å®Ÿè£…**:

```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
http_requests_total = Counter(
    "autoforge_http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status_code"]
)

http_request_duration_seconds = Histogram(
    "autoforge_http_request_duration_seconds",
    "HTTP request latency",
    ["method", "endpoint"],
    buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)
)

llm_tokens_total = Counter(
    "autoforge_llm_tokens_total",
    "Total LLM tokens used",
    ["provider", "model", "token_type"]  # token_type: prompt/completion
)

llm_cost_usd_total = Counter(
    "autoforge_llm_cost_usd_total",
    "Total LLM cost in USD",
    ["provider", "model"]
)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

#### 2.4 ã‚«ã‚¹ã‚¿ãƒ ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸è¶³

**å•é¡Œ**: LLMç‰¹åŒ–ã®ãƒ“ã‚¸ãƒã‚¹KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒä¸è¶³ã€‚

**æ¨å¥¨è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:

```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
prompt_quality_score = Histogram(
    "autoforge_prompt_quality_score",
    "Prompt quality score distribution",
    buckets=(0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99)
)

# è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
evaluation_duration_seconds = Histogram(
    "autoforge_evaluation_duration_seconds",
    "Evaluation execution time",
    ["test_suite_id", "parallel_count"]
)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ
user_sessions_active = Gauge(
    "autoforge_user_sessions_active",
    "Number of active user sessions"
)

# ã‚³ã‚¹ãƒˆåŠ¹ç‡
cost_per_quality_point = Gauge(
    "autoforge_cost_per_quality_point",
    "Cost per quality point (USD / quality_score)",
    ["organization_id"]
)
```

#### 2.5 SLI/SLOå®Ÿè£…ã®æº–å‚™ä¸è¶³

**å•é¡Œ**: Service Level Indicators/Objectivesã®å®Ÿè£…æº–å‚™ãŒãªã„ã€‚

**æ¨å¥¨SLIå®šç¾©**:

```python
# SLI: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
sli_prompt_response_time_p95 = Histogram(
    "autoforge_sli_prompt_response_time_seconds",
    "P95 prompt response time",
    buckets=(0.1, 0.5, 1.0, 2.0, 5.0, 10.0)
)

# SLO: 99.9%å¯ç”¨æ€§ç›®æ¨™
slo_availability_target = 0.999

# ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆè¨ˆç®—
def calculate_error_budget(total_requests: int, failed_requests: int) -> float:
    availability = (total_requests - failed_requests) / total_requests
    error_budget_remaining = 1 - (1 - availability) / (1 - slo_availability_target)
    return error_budget_remaining
```

**å‚è€ƒ**: BP#14ï¼ˆCLAUDE.mdãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ•ãƒ­ãƒ¼ï¼‰

---

## 3ï¸âƒ£ æ§‹é€ åŒ–ãƒ­ã‚°ã®ä¸€è²«æ€§

### âœ… å¼·ã¿ï¼ˆé«˜è©•ä¾¡é …ç›®ï¼‰

#### 3.1 TypedDictå‹å®‰å…¨æ€§

```python
# observability.py:26-79
class RequestContext(TypedDict, total=False):
    request_id: str
    timestamp: str
    method: str
    path: str
    ...

class LLMCallContext(TypedDict, total=False):
    call_id: str
    provider: str
    model: str
    ...
```

**è©•ä¾¡**: å‹å®‰å…¨æ€§ã«ã‚ˆã‚Šã€ãƒ­ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ä¸€è²«æ€§ã¨IDEã‚µãƒãƒ¼ãƒˆã‚’å®Ÿç¾ã€‚**æ¥­ç•Œãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹ **ã€‚

#### 3.2 JSONæ§‹é€ åŒ–ãƒ­ã‚°è¨­å®š

```python
# observability.py:500-504
"formatters": {
    "json": {
        "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
        "format": "%(asctime)s %(name)s %(levelname)s %(message)s ..."
    }
}
```

**è©•ä¾¡**: Lokiã€Elasticsearchç­‰ã®ãƒ­ã‚°é›†ç´„ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµ±åˆãŒå®¹æ˜“ã€‚

#### 3.3 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é…æ…®ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³

```python
# observability.py:262-286
def _sanitize_headers(self, headers: dict[str, str]) -> dict[str, str]:
    for key, value in headers.items():
        if key.lower() in self.sensitive_headers:
            sanitized[key] = "[REDACTED]"

def _sanitize_body(self, body: bytes) -> str:
    # JSONæ©Ÿå¯†ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’REDACTEDåŒ–
    sensitive_keys = ["password", "token", "secret", "key", ...]
```

**è©•ä¾¡**:
GDPRæº–æ‹ ã€PIIï¼ˆå€‹äººè­˜åˆ¥æƒ…å ±ï¼‰ä¿è­·ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»å¯¾å¿œã€‚**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã®è¨­è¨ˆ**ã€‚

#### 3.4 DoSæ”»æ’ƒå¯¾ç­–

```python
# observability.py:295-298
# Prevent deep nesting DoS attacks
max_depth = 10
if depth > max_depth:
    return {"error": "[DEPTH_LIMIT_EXCEEDED]"}
```

**è©•ä¾¡**: æ‚ªæ„ã‚ã‚‹ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªæ¯æ¸‡ã‚’é˜²æ­¢ã€‚

### âš ï¸ æ”¹å–„ãŒå¿…è¦ãªé …ç›®

#### 3.5 ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«æˆ¦ç•¥ã®æ˜ç¢ºåŒ–ä¸è¶³

**å•é¡Œ**: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®ä½¿ã„åˆ†ã‘åŸºæº–ãŒä¸æ˜ç¢ºã€‚

**æ¨å¥¨ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**:

```python
# ERROR: å³åº§ã®å¯¾å¿œãŒå¿…è¦ï¼ˆãƒšãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰
logger.error("Database connection lost", extra={"context": ...}, exc_info=True)

# WARNING: æ³¨æ„ãŒå¿…è¦ã ãŒç¶™ç¶šå¯èƒ½ï¼ˆãƒ¡ãƒ¼ãƒ«é€šçŸ¥ï¼‰
logger.warning("Slow query detected: 1.5s", extra={"context": ...})

# INFO: é‡è¦ãªçŠ¶æ…‹å¤‰åŒ–ï¼ˆãƒ­ã‚°é›†ç´„ã®ã¿ï¼‰
logger.info("Prompt optimization completed", extra={"context": ...})

# DEBUG: è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
logger.debug("LLM request payload", extra={"payload": ...})
```

#### 3.6 ãƒ­ã‚°ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æœªå®Ÿè£…

**å•é¡Œ**: é«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯æ™‚ã®ãƒ­ã‚°é‡çˆ†ç™ºã¸ã®å¯¾ç­–ãŒãªã„ã€‚

**æ¨å¥¨å®Ÿè£…**:

```python
import random

class SamplingLogger:
    def __init__(self, logger: logging.Logger, sample_rate: float = 0.1):
        self.logger = logger
        self.sample_rate = sample_rate

    def info(self, msg: str, **kwargs: Any) -> None:
        if random.random() < self.sample_rate:
            self.logger.info(msg, **kwargs)

    # WARNING/ERROR ã¯å¸¸ã«ãƒ­ã‚°
    def warning(self, msg: str, **kwargs: Any) -> None:
        self.logger.warning(msg, **kwargs)

# ä½¿ç”¨ä¾‹
sampled_logger = SamplingLogger(logger, sample_rate=0.05)  # 5%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
```

---

## 4ï¸âƒ£ LangFuseçµ±åˆã®æ­£ç¢ºæ€§

### âœ… å¼·ã¿

#### 4.1 LangFuseæ¥ç¶šæº–å‚™

```python
# monitoring.py:291-325
async def _check_langfuse(self) -> DependencyHealth:
    langfuse_host = os.getenv("LANGFUSE_HOST")
    async with httpx.AsyncClient(timeout=5.0) as client:
        response = await client.get(f"{langfuse_host}/api/public/health")
```

**è©•ä¾¡**: LangFuseãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãŒå®Ÿè£…ã•ã‚Œã€æ¥ç¶šæ€§ç¢ºèªãŒå¯èƒ½ã€‚

#### 4.2 LLMãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æº–å‚™

```python
# observability.py:326-383
class LLMObservabilityMiddleware:
    async def track_llm_call(...):
        context: LLMCallContext = {
            "call_id": call_id,
            "provider": provider,
            "model": model,
            ...
        }
```

**è©•ä¾¡**: LangFuseã«å¿…è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆcall_idã€providerã€modelï¼‰ã‚’åé›†ã€‚

### âš ï¸ æ”¹å–„ãŒå¿…è¦ãªé …ç›®ï¼ˆCriticalï¼‰

#### 4.3 LangFuse SDKæœªçµ±åˆ

**å•é¡Œ**: LangFuseå…¬å¼SDKãŒä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã€‚

**æ¨å¥¨å®Ÿè£…**:

```python
from langfuse import Langfuse
from langfuse.decorators import observe

langfuse = Langfuse(
    public_key=settings.langfuse_public_key,
    secret_key=settings.langfuse_secret_key,
    host=settings.langfuse_host,
)

@observe()  # è‡ªå‹•ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°
async def generate_prompt(user_input: UserInput) -> str:
    # LangFuseãŒè‡ªå‹•çš„ã«ãƒˆãƒ¬ãƒ¼ã‚¹ä½œæˆ
    llm_response = await litellm.acompletion(
        model="gpt-4",
        messages=[{"role": "user", "content": user_input.goal}]
    )
    return llm_response.choices[0].message.content

# ã¾ãŸã¯æ‰‹å‹•ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°
trace = langfuse.trace(
    name="prompt-optimization",
    user_id="user_123",
    session_id="session_456",
    metadata={"organization_id": "org_789"}
)

span = trace.span(
    name="llm-generation",
    metadata={"provider": "openai", "model": "gpt-4"}
)
span.end(output=llm_response)
```

**å‚è€ƒ**: [LangFuse Python SDK](https://langfuse.com/docs/sdk/python)

#### 4.4 éšå±¤ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ä¸è¶³ï¼ˆBP#1é•åï¼‰

**å•é¡Œ**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒã‚§ãƒ¼ãƒ³å…¨ä½“ã®éšå±¤æ§‹é€ ãƒˆãƒ¬ãƒ¼ã‚¹ãŒãªã„ã€‚

**æ¨å¥¨å®Ÿè£…ï¼ˆBP#1æº–æ‹ ï¼‰**:

```python
# BP#1: LangFuseéšå±¤ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å®Ÿè£…
trace = langfuse.trace(name="prompt-optimization-flow")

# Step 1: Template Analysis
with trace.span(name="template_analysis") as span:
    result = await analyze_template(user_input)
    span.update(
        metadata={"template_count": len(result.templates)},
        output=result.to_dict()
    )

# Step 2: Provider Evaluation
with trace.span(name="provider_evaluation") as span:
    providers = await evaluate_providers(result.templates)
    span.update(
        metadata={"providers_tested": len(providers)},
        output=providers
    )

# Step 3: Optimization
with trace.span(name="optimization") as span:
    optimized = await optimize_prompt(providers[0])
    span.update(
        metadata={
            "tokens": optimized.tokens,
            "cost": optimized.cost,
            "quality_score": optimized.quality
        }
    )
```

#### 4.5 ã‚³ã‚¹ãƒˆç›£è¦–æœªå®Ÿè£…ï¼ˆBP#2é•åï¼‰

**å•é¡Œ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚³ã‚¹ãƒˆç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆãŒãªã„ã€‚

**æ¨å¥¨å®Ÿè£…ï¼ˆBP#2æº–æ‹ ï¼‰**:

```python
# BP#2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚³ã‚¹ãƒˆç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ
class CostMonitor:
    def __init__(self, monthly_budget_usd: float):
        self.monthly_budget = monthly_budget_usd
        self.current_spend = 0.0

    async def track_llm_cost(
        self, tokens: int, cost_per_token: float, provider: str
    ) -> None:
        cost = tokens * cost_per_token
        self.current_spend += cost

        # æ™‚é–“ãƒ¬ãƒ¼ãƒˆè¨ˆç®—ï¼ˆæœˆé–“äºˆæ¸¬ï¼‰
        days_elapsed = datetime.now().day
        monthly_projection = (self.current_spend / days_elapsed) * 30

        # 90%é–¾å€¤ã§äº‹å‰ã‚¢ãƒ©ãƒ¼ãƒˆ
        if monthly_projection > self.monthly_budget * 0.9:
            logger.warning(
                "Budget alert: 90% threshold reached",
                extra={
                    "current_spend": self.current_spend,
                    "monthly_projection": monthly_projection,
                    "budget": self.monthly_budget,
                    "provider": provider
                }
            )
            # Slack/Discordé€šçŸ¥
            await send_alert(f"LLM cost approaching budget: ${monthly_projection:.2f}")
```

---

## 5ï¸âƒ£ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã®å®Ÿè£…

### âœ… å¼·ã¿

#### 5.1 ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªæ¤œçŸ¥

```python
# observability.py:439-442
if duration > 1.0:  # 1ç§’ä»¥ä¸Š
    self.logger.warning("Slow database query detected", ...)
```

**è©•ä¾¡**: 1ç§’é–¾å€¤ã§è‡ªå‹•æ¤œçŸ¥ã€ç¶™ç¶šçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®èµ·ç‚¹ã€‚

#### 5.2 ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ è¨˜éŒ²

```python
# observability.py:161, 213
duration = time.time() - start_time
response.headers["X-Response-Time"] = str(int(duration * 1000))
```

**è©•ä¾¡**: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã§ã‚‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ç¢ºèªå¯èƒ½ã€‚

### âš ï¸ æ”¹å–„ãŒå¿…è¦ãªé …ç›®

#### 5.3 ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—ä¸è¶³

**å•é¡Œ**: P50ã€P95ã€P99ãªã©ã®ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒãªã„ã€‚

**æ¨å¥¨å®Ÿè£…**:

```python
from prometheus_client import Histogram

http_request_duration_seconds = Histogram(
    "autoforge_http_request_duration_seconds",
    "HTTP request latency in seconds",
    ["method", "endpoint"],
    buckets=(0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 7.5, 10.0)
)

# Grafanaã‚¯ã‚¨ãƒªä¾‹
# P95: histogram_quantile(0.95, rate(autoforge_http_request_duration_seconds_bucket[5m]))
# P99: histogram_quantile(0.99, rate(autoforge_http_request_duration_seconds_bucket[5m]))
```

#### 5.4 ç¶™ç¶šçš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°æœªå®Ÿè£…ï¼ˆBP#12é•åï¼‰

**å•é¡Œ**: CPU/ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãŒãªã„ã€‚

**æ¨å¥¨å®Ÿè£…ï¼ˆBP#12æº–æ‹ ï¼‰**:

```python
# BP#12: ç¶™ç¶šçš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµ±åˆ
import tracemalloc
import cProfile
import pstats
from io import StringIO

# ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
tracemalloc.start()

async def periodic_memory_snapshot():
    while True:
        await asyncio.sleep(300)  # 5åˆ†ã”ã¨
        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')

        for stat in top_stats[:10]:
            if stat.size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                logger.warning(
                    "Memory leak detected",
                    extra={"file": stat.traceback, "size_mb": stat.size / 1024 / 1024}
                )

# CPU ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼ˆ5%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
@app.middleware("http")
async def profile_middleware(request: Request, call_next):
    if random.random() < 0.05:  # 5%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        profiler = cProfile.Profile()
        profiler.enable()
        response = await call_next(request)
        profiler.disable()

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«çµæœã‚’ä¿å­˜
        s = StringIO()
        ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        ps.print_stats(20)
        logger.debug("CPU profile", extra={"profile": s.getvalue()})
        return response
    return await call_next(request)
```

---

## 6ï¸âƒ£ ã‚¨ãƒ©ãƒ¼è¿½è·¡ã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

### âœ… å¼·ã¿

#### 6.1 è©³ç´°ãªã‚¨ãƒ©ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

```python
# observability.py:221-238
error_context: ErrorContext = {
    **context,
    "duration_ms": duration * 1000,
    "error": str(e),
    "error_type": type(e).__name__,
}

logger.error(
    "Request failed with exception",
    extra={"context": error_context},
    exc_info=True  # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹å«ã‚€
)
```

**è©•ä¾¡**: ã‚¨ãƒ©ãƒ¼å†ç¾ã«å¿…è¦ãªæƒ…å ±ã‚’ç¶²ç¾…ã€ãƒ‡ãƒãƒƒã‚°åŠ¹ç‡å‘ä¸Šã€‚

#### 6.2 ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²

```python
# monitoring.py:444-459
def record_error_metrics(
    self, error_type: str, error_message: str, stack_trace: str | None = None
) -> None:
    metric = {
        "type": "error",
        "error_type": error_type,
        "error_message": error_message,
        "stack_trace": stack_trace,
    }
```

**è©•ä¾¡**: ã‚¨ãƒ©ãƒ¼åˆ†é¡ã€å‚¾å‘åˆ†æã€å½±éŸ¿ç¯„å›²ç‰¹å®šãŒå¯èƒ½ã€‚

### âš ï¸ æ”¹å–„ãŒå¿…è¦ãªé …ç›®ï¼ˆCriticalï¼‰

#### 6.3 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šæœªå®Ÿè£…

**å•é¡Œ**: Prometheus Alertmanagerã‚„Sentryçµ±åˆãŒãªã„ã€‚

**æ¨å¥¨å®Ÿè£…**:

```python
# Sentryçµ±åˆ
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration

sentry_sdk.init(
    dsn=settings.sentry_dsn,
    integrations=[FastApiIntegration()],
    traces_sample_rate=0.1,  # 10%ãƒˆãƒ¬ãƒ¼ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    environment=settings.app_env,
    release=f"autoforgenexus@{settings.version}",
)

# ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆ
if error_rate > 0.05:  # 5%ä»¥ä¸Š
    sentry_sdk.capture_message(
        "High error rate detected",
        level="error",
        extras={"error_rate": error_rate, "endpoint": endpoint}
    )
```

**Prometheus Alertmanagerè¨­å®šä¾‹**:

```yaml
# prometheus-alerts.yml
groups:
  - name: autoforgenexus
    rules:
      - alert: HighErrorRate
        expr: rate(autoforge_http_requests_total{status_code=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: 'High error rate ({{ $value }})'

      - alert: SlowPromptGeneration
        expr:
          histogram_quantile(0.95,
          rate(autoforge_llm_duration_seconds_bucket[5m])) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: 'P95 prompt generation > 5s'
```

#### 6.4 ç•°å¸¸æ¤œçŸ¥æœªå®Ÿè£…ï¼ˆBP#15é•åï¼‰

**å•é¡Œ**: äºˆæ¸¬çš„ã‚¢ãƒ©ãƒ¼ãƒˆã€ãƒ‘ã‚¿ãƒ¼ãƒ³ç•°å¸¸æ¤œçŸ¥ãŒãªã„ã€‚

**æ¨å¥¨å®Ÿè£…ï¼ˆBP#15æº–æ‹ ï¼‰**:

```python
# BP#15: äºˆæ¸¬çš„ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
from sklearn.ensemble import IsolationForest
import numpy as np

class AnomalyDetector:
    def __init__(self, contamination: float = 0.1):
        self.model = IsolationForest(contamination=contamination)
        self.history: list[dict[str, float]] = []

    def detect_anomaly(self, metrics: dict[str, float]) -> bool:
        self.history.append(metrics)

        if len(self.history) < 100:  # æœ€ä½100ã‚µãƒ³ãƒ—ãƒ«å¿…è¦
            return False

        X = np.array([[m["duration_ms"], m["tokens_used"], m["cost"]]
                      for m in self.history])
        self.model.fit(X)

        current = np.array([[metrics["duration_ms"],
                            metrics["tokens_used"],
                            metrics["cost"]]])
        prediction = self.model.predict(current)

        if prediction[0] == -1:  # ç•°å¸¸æ¤œçŸ¥
            logger.warning(
                "Anomaly detected in LLM metrics",
                extra={"metrics": metrics, "history_size": len(self.history)}
            )
            return True
        return False

# ã‚³ã‚¹ãƒˆè¶…éäºˆæ¸¬
def predict_monthly_cost(daily_costs: list[float]) -> float:
    from sklearn.linear_model import LinearRegression
    X = np.array(range(len(daily_costs))).reshape(-1, 1)
    y = np.array(daily_costs)
    model = LinearRegression().fit(X, y)

    # 30æ—¥å¾Œäºˆæ¸¬
    return model.predict([[30]])[0]
```

---

## 7ï¸âƒ£ è¦³æ¸¬æ€§ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹ 

### âœ… å¼·ã¿

#### 7.1 Three Pillars of Observability

- âœ… **Logs**: æ§‹é€ åŒ–ãƒ­ã‚°å®Œå‚™
- âœ… **Metrics**: ã‚·ã‚¹ãƒ†ãƒ ãƒ»HTTPãƒ»LLMãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- âš ï¸ **Traces**: åŸºç¤å®Ÿè£…ã‚ã‚Šã€OpenTelemetryçµ±åˆå¾…ã¡

#### 7.2 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆè¨­è¨ˆ

- âœ… æ©Ÿå¯†æƒ…å ±ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
- âœ… DoSæ”»æ’ƒå¯¾ç­–ï¼ˆdepthåˆ¶é™ï¼‰
- âœ… PIIä¿è­·ï¼ˆGDPRæº–æ‹ ï¼‰

#### 7.3 å‹å®‰å…¨æ€§

- âœ… TypedDictä½¿ç”¨
- âœ… mypy strictãƒ¢ãƒ¼ãƒ‰å¯¾å¿œæº–å‚™

### âš ï¸ æ”¹å–„ãŒå¿…è¦ãªé …ç›®

#### 7.4 2025å¹´æ¥­ç•Œæ¨™æº–ã¸ã®æº–æ‹ ä¸è¶³

**å•é¡Œç‚¹**:

1. OpenTelemetryæœªä½¿ç”¨ï¼ˆæ¥­ç•Œæ¨™æº–ï¼‰
2. LangFuse OTel Native SDKæœªçµ±åˆï¼ˆ2025å¹´æœ€æ–°ï¼‰
3. Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªå®Ÿè£…
4. SLI/SLOæœªå®šç¾©

**æ¨å¥¨å¯¾å¿œ**:

- BP#4: GenAIã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¦ç´„æº–æ‹ 
- BP#5: ãƒãƒƒãƒã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–
- BP#6: åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°çµ±åˆï¼ˆNext.js â†’ Cloudflare Workers â†’ FastAPIï¼‰

---

## ğŸ“Š æ”¹å–„å„ªå…ˆé †ä½ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

### Criticalï¼ˆå¿…é ˆå¯¾å¿œã€Week 1-2ï¼‰

| é …ç›®                        | å½±éŸ¿ç¯„å›²        | å®Ÿè£…å·¥æ•° | ROI  |
| --------------------------- | --------------- | -------- | ---- |
| 1. OpenTelemetryçµ±åˆ        | ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“    | 2-3æ—¥    | è¶…é«˜ |
| 2. LangFuse SDKçµ±åˆ         | LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | 1-2æ—¥    | é«˜   |
| 3. Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£… | ç›£è¦–åŸºç›¤        | 1æ—¥      | è¶…é«˜ |
| 4. ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šï¼ˆSentryï¼‰   | é‹ç”¨å“è³ª        | 1æ—¥      | é«˜   |

### Highï¼ˆæ¨å¥¨å¯¾å¿œã€Week 3-4ï¼‰

| é …ç›®                      | å½±éŸ¿ç¯„å›²       | å®Ÿè£…å·¥æ•° | ROI |
| ------------------------- | -------------- | -------- | --- |
| 5. SLI/SLOå®Ÿè£…            | å“è³ªä¿è¨¼       | 2æ—¥      | é«˜  |
| 6. ã‚³ã‚¹ãƒˆç›£è¦–ï¼ˆBP#2ï¼‰     | ã‚³ã‚¹ãƒˆæœ€é©åŒ–   | 1æ—¥      | ä¸­  |
| 7. ç•°å¸¸æ¤œçŸ¥ï¼ˆBP#15ï¼‰      | äºˆé˜²ä¿å®ˆ       | 2-3æ—¥    | ä¸­  |
| 8. ç¶™ç¶šçš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | 1-2æ—¥    | ä¸­  |

### Mediumï¼ˆæ®µéšçš„å¯¾å¿œã€Week 5-6ï¼‰

| é …ç›®                         | å½±éŸ¿ç¯„å›²   | å®Ÿè£…å·¥æ•° | ROI |
| ---------------------------- | ---------- | -------- | --- |
| 9. ãƒ­ã‚°ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°          | ã‚³ã‚¹ãƒˆå‰Šæ¸› | 0.5æ—¥    | ä½  |
| 10. ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | è©³ç´°åˆ†æ   | 0.5æ—¥    | ä½  |

---

## ğŸ¯ å…·ä½“çš„ãªå®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Week 1: OpenTelemetryåŸºç›¤æ§‹ç¯‰

**Day 1-2: OpenTelemetry SDKçµ±åˆ**

```python
# requirements.txt
opentelemetry-api==1.25.0
opentelemetry-sdk==1.25.0
opentelemetry-instrumentation-fastapi==0.46b0
opentelemetry-instrumentation-sqlalchemy==0.46b0
opentelemetry-exporter-otlp==1.25.0

# backend/src/core/observability/otel.py
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

def setup_otel(app: FastAPI) -> None:
    provider = TracerProvider()
    processor = BatchSpanProcessor(
        OTLPSpanExporter(
            endpoint="https://cloud.langfuse.com/api/public/ingestion",
            headers={"Authorization": f"Bearer {settings.langfuse_secret_key}"}
        ),
        max_queue_size=2048,
        max_export_batch_size=512,
        schedule_delay_millis=5000,
        export_timeout_millis=30000
    )
    provider.add_span_processor(processor)
    trace.set_tracer_provider(provider)

    FastAPIInstrumentor.instrument_app(app)
```

**Day 3: LangFuse SDKçµ±åˆ**

```python
# backend/src/infrastructure/llm_integration/langfuse_client.py
from langfuse import Langfuse
from langfuse.decorators import observe

langfuse = Langfuse(
    public_key=settings.langfuse_public_key,
    secret_key=settings.langfuse_secret_key,
    host=settings.langfuse_host
)

@observe()
async def generate_optimized_prompt(user_input: UserInput) -> str:
    trace = langfuse.trace(
        name="prompt-optimization",
        user_id=user_input.user_id,
        metadata={"organization_id": user_input.organization_id}
    )

    # Step 1: Template Analysis
    with trace.span(name="template_analysis") as span:
        templates = await analyze_templates(user_input)
        span.update(output=templates)

    # Step 2-3: ä»¥ä¸‹åŒæ§˜
    ...
```

### Week 2: Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

**Day 1: Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…**

```python
# backend/src/core/observability/prometheus.py
from prometheus_client import Counter, Histogram, Gauge

http_requests_total = Counter(
    "autoforge_http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status_code"]
)

llm_tokens_total = Counter(
    "autoforge_llm_tokens_total",
    "Total LLM tokens",
    ["provider", "model", "token_type"]
)

llm_cost_usd_total = Counter(
    "autoforge_llm_cost_usd_total",
    "Total LLM cost",
    ["provider", "model"]
)

prompt_quality_score = Histogram(
    "autoforge_prompt_quality_score",
    "Prompt quality distribution",
    buckets=(0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99)
)
```

**Day 2: Sentryã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š**

```python
# backend/src/core/observability/sentry.py
import sentry_sdk

sentry_sdk.init(
    dsn=settings.sentry_dsn,
    environment=settings.app_env,
    release=f"autoforgenexus@{settings.version}",
    traces_sample_rate=0.1,
    profiles_sample_rate=0.1,
    integrations=[FastApiIntegration()]
)
```

### Week 3-4: SLI/SLOã¨ã‚³ã‚¹ãƒˆç›£è¦–

**Day 1-2: SLI/SLOå®Ÿè£…**

```python
# backend/src/core/observability/slo.py
class SLOMonitor:
    def __init__(self):
        self.slo_targets = {
            "prompt_response_time_p95": 2.0,  # 2ç§’
            "llm_service_availability": 0.999,  # 99.9%
            "evaluation_accuracy": 0.90,  # 90%
            "cost_efficiency": 0.85  # 85%
        }

    def calculate_error_budget(self, sli_name: str, actual_value: float) -> float:
        target = self.slo_targets[sli_name]
        return 1 - (target - actual_value) / (1 - target)
```

**Day 3-4: ã‚³ã‚¹ãƒˆç›£è¦–å®Ÿè£…ï¼ˆBP#2ï¼‰**

```python
# backend/src/core/observability/cost_monitor.py
class CostMonitor:
    async def track_and_alert(self, cost: float, provider: str) -> None:
        monthly_projection = self.predict_monthly_cost()
        if monthly_projection > self.budget * 0.9:
            await self.send_alert(f"Budget 90% reached: ${monthly_projection}")
```

---

## ğŸ” æ¨å¥¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ

### Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹æˆï¼ˆBP#13, BP#14æº–æ‹ ï¼‰

#### Panel 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ•ãƒ­ãƒ¼å…¨ä½“å¯è¦–åŒ–ï¼ˆBP#13ï¼‰

```promql
# Template Usage
sum(rate(autoforge_template_usage_total[5m])) by (template_id)

# Optimization Iterations
histogram_quantile(0.95,
  rate(autoforge_optimization_iterations_bucket[5m])
)

# Quality Improvement
rate(autoforge_quality_improvement_total[5m])
```

#### Panel 2: SLI/SLOè‡ªå‹•è¿½è·¡ï¼ˆBP#14ï¼‰

```promql
# Prompt Response Time P95
histogram_quantile(0.95,
  rate(autoforge_sli_prompt_response_time_seconds_bucket[1h])
)

# LLM Service Availability
1 - (
  rate(autoforge_http_requests_total{status_code=~"5.."}[24h])
  /
  rate(autoforge_http_requests_total[24h])
)

# Error Budget Consumption
(1 - autoforge_slo_availability) / (1 - 0.999) * 100
```

#### Panel 3: ã‚³ã‚¹ãƒˆè¿½è·¡ï¼ˆBP#2ï¼‰

```promql
# Real-time Cost
sum(rate(autoforge_llm_cost_usd_total[5m])) by (provider)

# Monthly Projection
sum(rate(autoforge_llm_cost_usd_total[30d])) * 30

# Cost per Quality Point
sum(rate(autoforge_llm_cost_usd_total[5m]))
/
avg(autoforge_prompt_quality_score)
```

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¦ã‚£ãƒ³ï¼ˆå³åŠ¹æ€§ã®ã‚ã‚‹æ”¹å–„ï¼‰

### 1. æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¸ã®å³åº§è¿½åŠ ï¼ˆ30åˆ†ï¼‰

```python
# backend/src/middleware/observability.pyï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ ï¼‰

# === è¿½åŠ 1: Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ===
from prometheus_client import Counter, Histogram, generate_latest
from fastapi.responses import PlainTextResponse

http_requests_total = Counter(
    "autoforge_http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status_code"]
)

# ObservabilityMiddleware.dispatchå†…ã«è¿½åŠ 
http_requests_total.labels(
    method=request.method,
    endpoint=request.url.path,
    status_code=response.status_code
).inc()

# main.pyã«è¿½åŠ 
@app.get("/metrics", response_class=PlainTextResponse)
async def metrics():
    return generate_latest()
```

### 2. LangFuseæœ€å°çµ±åˆï¼ˆ1æ™‚é–“ï¼‰

```python
# backend/src/infrastructure/llm_integration/langfuse_minimal.py
from langfuse import Langfuse

langfuse = Langfuse(
    public_key=settings.langfuse_public_key,
    secret_key=settings.langfuse_secret_key
)

# æ—¢å­˜ã®LLMå‘¼ã³å‡ºã—ã‚’ãƒ©ãƒƒãƒ—
async def track_llm_call_langfuse(provider: str, model: str, prompt: str):
    trace = langfuse.trace(name="llm-call")
    generation = trace.generation(
        name=f"{provider}-{model}",
        model=model,
        input=prompt
    )

    response = await litellm.acompletion(model=model, messages=[{"role": "user", "content": prompt}])

    generation.end(
        output=response.choices[0].message.content,
        usage={
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens
        }
    )

    return response
```

### 3. Sentryã‚¨ãƒ©ãƒ¼è¿½è·¡ï¼ˆ30åˆ†ï¼‰

```python
# backend/src/main.py
import sentry_sdk

sentry_sdk.init(
    dsn=settings.sentry_dsn,
    environment=settings.app_env,
    traces_sample_rate=0.1
)

# ã“ã‚Œã ã‘ã§å…¨ã‚¨ãƒ©ãƒ¼ãŒè‡ªå‹•åé›†ã•ã‚Œã‚‹
```

**åˆè¨ˆå·¥æ•°**: 2æ™‚é–“ã§3ã¤ã®é‡è¦æ©Ÿèƒ½è¿½åŠ å¯èƒ½

---

## ğŸ“ çµè«–ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼

AutoForgeNexusã®è¦³æ¸¬æ€§å®Ÿè£…ã¯ã€**LLMç‰¹åŒ–ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦åŸºç¤ã‚’ç¢ºç«‹**ã—ã¦ãŠã‚Šã€ä»¥ä¸‹ã®ç‚¹ã§å„ªã‚Œã¦ã„ã¾ã™:

1. âœ… **å‹å®‰å…¨ãªæ§‹é€ åŒ–ãƒ­ã‚°**ï¼ˆTypedDictã€JSONå½¢å¼ï¼‰
2. âœ… **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã®è¨­è¨ˆ**ï¼ˆPIIä¿è­·ã€DoSå¯¾ç­–ï¼‰
3. âœ… **åŒ…æ‹¬çš„ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯**ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã€ä¾å­˜é–¢ä¿‚ï¼‰
4. âœ… **LLMç‰¹åŒ–ã®è¦³æ¸¬æº–å‚™**ï¼ˆcall_idã€providerã€modelè¿½è·¡ï¼‰

ãŸã ã—ã€æœ¬ç•ªé‹ç”¨ã«å‘ã‘ã¦ä»¥ä¸‹ã®**Criticalé …ç›®ã®å¯¾å¿œãŒå¿…é ˆ**ã§ã™:

### å¿…é ˆå¯¾å¿œé …ç›®ï¼ˆå„ªå…ˆé †ä½é †ï¼‰

1. **ğŸ”´ OpenTelemetryçµ±åˆ**ï¼ˆWeek 1ï¼‰

   - æ¥­ç•Œæ¨™æº–ã¸ã®æº–æ‹ 
   - Jaeger/Zipkin/Tempoé€£æº
   - GenAIã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¦ç´„é©ç”¨

2. **ğŸ”´ LangFuse SDKçµ±åˆ**ï¼ˆWeek 1ï¼‰

   - éšå±¤ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ï¼ˆBP#1ï¼‰
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒã‚§ãƒ¼ãƒ³å¯è¦–åŒ–
   - ã‚³ã‚¹ãƒˆç›£è¦–ï¼ˆBP#2ï¼‰

3. **ğŸ”´ Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…**ï¼ˆWeek 2ï¼‰

   - Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
   - SLI/SLOç›£è¦–ï¼ˆBP#14ï¼‰
   - ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

4. **ğŸ”´ äºˆæ¸¬çš„ã‚¢ãƒ©ãƒ¼ãƒˆ**ï¼ˆWeek 3-4ï¼‰
   - ç•°å¸¸æ¤œçŸ¥ï¼ˆBP#15ï¼‰
   - ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆè¨ˆç®—
   - æœˆé–“ã‚³ã‚¹ãƒˆäºˆæ¸¬

### æ¨å¥¨å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| Week   | å®Ÿè£…é …ç›®                     | æˆæœç‰©                   | å·¥æ•° |
| ------ | ---------------------------- | ------------------------ | ---- |
| Week 1 | OpenTelemetry + LangFuseåŸºç›¤ | åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å‹•ä½œ     | 3æ—¥  |
| Week 2 | Prometheus + Sentry          | ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ | 2æ—¥  |
| Week 3 | SLI/SLO + ã‚³ã‚¹ãƒˆç›£è¦–         | å“è³ªãƒ»ã‚³ã‚¹ãƒˆè¿½è·¡         | 2æ—¥  |
| Week 4 | ç•°å¸¸æ¤œçŸ¥ + äºˆæ¸¬æ©Ÿèƒ½          | äºˆé˜²çš„é‹ç”¨               | 2æ—¥  |

**åˆè¨ˆå·¥æ•°**: 9æ—¥é–“ï¼ˆç´„2é€±é–“ï¼‰ã§æœ¬ç•ªé‹ç”¨ãƒ¬ãƒ™ãƒ«é”æˆ

### æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆ4é€±é–“å¾Œï¼‰

- âœ… OpenTelemetryçµ±åˆç‡: 100%ï¼ˆå…¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼‰
- âœ… LangFuseéšå±¤ãƒˆãƒ¬ãƒ¼ã‚¹: 100%ï¼ˆLLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ï¼‰
- âœ… Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹: 50+é …ç›®
- âœ… Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: 3ç”»é¢ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã€SLOã€ã‚³ã‚¹ãƒˆï¼‰
- âœ… ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š: 10+ãƒ«ãƒ¼ãƒ«
- âœ… MTTRï¼ˆå¹³å‡å¾©æ—§æ™‚é–“ï¼‰: 50%å‰Šæ¸›ï¼ˆç¾çŠ¶æ¨å®š30åˆ† â†’ 15åˆ†ï¼‰

### é•·æœŸçš„ãªè¦³æ¸¬æ€§æˆ¦ç•¥ï¼ˆPhase 4-6ï¼‰

#### Phase 4: AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¦³æ¸¬å¯èƒ½æ€§ï¼ˆBP#18ï¼‰

- ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ç›£è¦–
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°
- æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–

#### Phase 5: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€é©åŒ–ï¼ˆBP#9ï¼‰

- åœ°åŸŸåˆ¥SLOè¨­å®šï¼ˆUS<100msã€EU<150msã€APAC<200msï¼‰
- Cloudflare Workersåˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°
- Tursoãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ç›£è¦–

#### Phase 6: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦³æ¸¬å¯èƒ½æ€§ï¼ˆBP#20ï¼‰

- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ¤œå‡º
- APIã‚­ãƒ¼ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç•°å¸¸æ¤œçŸ¥
- GDPR/CCPAã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ç›£æŸ»ãƒ­ã‚°

---

**æœ€çµ‚æ¨å¥¨**: æœ¬ãƒ¬ãƒãƒ¼ãƒˆã§ç‰¹å®šã•ã‚ŒãŸ**Criticalé …ç›®4ã¤**ã«å„ªå…ˆçš„ã«å–ã‚Šçµ„ã‚€ã“ã¨ã§ã€AutoForgeNexusã¯**99.9%å¯ç”¨æ€§ã€P95<2ç§’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€æœˆé–“ã‚³ã‚¹ãƒˆäºˆæ¸¬ç²¾åº¦95%ä»¥ä¸Š**ã‚’é”æˆã—ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®è¦³æ¸¬æ€§åŸºç›¤ã‚’ç¢ºç«‹ã§ãã¾ã™ã€‚

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: Week 1ã®å®Ÿè£…ã‹ã‚‰ç€æ‰‹ã—ã€OpenTelemetry +
LangFuseåŸºç›¤ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚

---

**ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ**: observability-engineer Agent **å‚è€ƒæ–‡çŒ®**:

- [Observability Engineering (2022)](https://www.oreilly.com/library/view/observability-engineering/9781492076438/)
- [Mastering Distributed Tracing (2019)](https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464)
- [LangFuse Documentation 2025](https://langfuse.com/docs)
- [OpenTelemetry Semantic Conventions for GenAI](https://opentelemetry.io/docs/specs/semconv/gen-ai/)
