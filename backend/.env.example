# ==========================================
# Backend Environment Variables Example
# ==========================================
# Copy to .env.local and replace placeholder values
# DO NOT commit .env.local - only commit this .env.example

# === Application Settings ===
APP_NAME=AutoForgeNexus-Backend
APP_ENV=local  # local | staging | production
DEBUG=true     # Set to false in production
LOG_LEVEL=DEBUG  # DEBUG | INFO | WARNING | ERROR
PORT=8000
HOST=0.0.0.0


# === Database Configuration ===
# For local development (SQLite)
DATABASE_URL=sqlite:///./data/local.db

# For Turso (Production/Staging)
# Get from: https://turso.tech/app/databases
TURSO_DATABASE_URL=libsql://your-database-name.turso.io
TURSO_AUTH_TOKEN=your-auth-token-from-turso-dashboard

# === Cache (Redis) ===
REDIS_HOST=localhost  # Use 'redis' for Docker
REDIS_PORT=6379
REDIS_PASSWORD=  # Leave empty for local
REDIS_DB=0
REDIS_CACHE_TTL=3600

# === Authentication ===
# Clerk (https://clerk.com/)
# Get from: Dashboard â†’ API Keys
CLERK_SECRET_KEY=sk_test_your-clerk-secret-key
CLERK_PUBLIC_KEY=pk_test_your-clerk-public-key
CLERK_WEBHOOK_SECRET=whsec_your-webhook-secret

# JWT Configuration
JWT_SECRET_KEY=generate-a-secure-random-key-here  # Use: openssl rand -hex 32
JWT_ALGORITHM=HS256
JWT_EXPIRATION_MINUTES=60



# === LLM Provider API Keys ===
# Get your API keys from respective providers:

# OpenAI - https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your-openai-api-key

# Anthropic - https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key

# Google AI - https://makersuite.google.com/app/apikey
GOOGLE_AI_API_KEY=your-google-ai-api-key

# Mistral - https://console.mistral.ai/
MISTRAL_API_KEY=your-mistral-api-key

# Cohere - https://dashboard.cohere.com/
COHERE_API_KEY=your-cohere-api-key

# Hugging Face - https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=hf_your-huggingface-token

# === LiteLLM Configuration ===
# LiteLLM Proxy settings
LITELLM_API_KEY=your-litellm-key-if-using-proxy
LITELLM_PROXY_BASE=http://localhost:4000
LITELLM_DEFAULT_MODEL=gpt-3.5-turbo  # or claude-3-haiku-20240307
LITELLM_FALLBACK_MODELS=  # comma-separated list
LITELLM_MAX_RETRIES=3
LITELLM_TIMEOUT=120

# === Observability ===
# LangFuse - https://langfuse.com/
LANGFUSE_HOST=http://localhost:3002  # or https://cloud.langfuse.com
LANGFUSE_PUBLIC_KEY=pk_your-langfuse-public-key
LANGFUSE_SECRET_KEY=sk_your-langfuse-secret-key

# === Security ===
# CORS Settings
CORS_ALLOW_ORIGINS=http://localhost:3000  # Frontend URL
CORS_ALLOW_CREDENTIALS=true
CORS_ALLOW_METHODS=GET,POST,PUT,DELETE,OPTIONS
CORS_ALLOW_HEADERS=*

# Rate Limiting
RATE_LIMIT_REQUESTS=100  # requests per period
RATE_LIMIT_PERIOD=60     # seconds

# === External Services ===
# Cloudflare - https://dash.cloudflare.com/profile/api-tokens
CLOUDFLARE_ACCOUNT_ID=your-account-id
CLOUDFLARE_API_TOKEN=your-api-token
CLOUDFLARE_ZONE_ID=your-zone-id

# S3/R2 Storage
S3_ACCESS_KEY_ID=your-access-key-id
S3_SECRET_ACCESS_KEY=your-secret-access-key
S3_BUCKET_NAME=your-bucket-name
S3_REGION=auto  # or specific region
S3_ENDPOINT_URL=  # Leave empty for AWS S3

# === Feature Flags ===
ENABLE_PROMPT_CACHING=true
ENABLE_VECTOR_SEARCH=false  # Requires vector DB setup
ENABLE_ASYNC_PROCESSING=false
ENABLE_RATE_LIMITING=false  # Enable in production
ENABLE_METRICS_COLLECTION=true

# === Performance ===
WORKER_PROCESSES=1  # Set to 'auto' in production
WORKER_CONNECTIONS=100
REQUEST_TIMEOUT=30
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# === Development Tools ===
RELOAD=true  # Auto-reload on code changes
RELOAD_DIRS=src,tests

# === Testing ===
TEST_DATABASE_URL=sqlite:///./data/test.db
TEST_REDIS_HOST=localhost
TEST_REDIS_PORT=6380  # Different port for test Redis